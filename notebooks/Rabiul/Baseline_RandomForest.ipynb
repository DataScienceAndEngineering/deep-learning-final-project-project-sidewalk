{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "hazyGcpGH8qS",
        "rvGsEQ-ZzA_T",
        "cCsykJ2qUSY7",
        "1rZorVuzutIt"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataScienceAndEngineering/deep-learning-final-project-project-sidewalk/blob/rabiul/notebooks/Rabiul/Baseline_RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Libraries"
      ],
      "metadata": {
        "id": "7QcascrI7M07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1SS4YIzG2fV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from skimage.filters import roberts, sobel, scharr, prewitt\n",
        "from scipy import ndimage as nd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "PO2TnZE37PmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading data and model \n",
        "!gdown --id 1-3gbEPrVnD0YZ38ZixZrRjo3y2-NeOgr -O test.csv\n",
        "!gdown --id 1TRDaaX13CqHE8xzIXKPnhLEhniWlUR9a -O train.csv\n",
        "!gdown --id 1-14yL80KGLJzF2D6j3BXTH1dLa2knz_W -O model.sav"
      ],
      "metadata": {
        "id": "iYZPEFo99R5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pickle.load(open('/content/model.sav', 'rb'))"
      ],
      "metadata": {
        "id": "vBhe37Xg9SZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_data('/content/train.csv', '/content/test.csv')\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "1rvqpXEQ9Sp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rf_accuracy(model, X_test,y_test))"
      ],
      "metadata": {
        "id": "3mBoEgmf9Znl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rf_accuracy(model, X_train,y_train))"
      ],
      "metadata": {
        "id": "ytQBfmK79S5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "GrwWFosL6HVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Loading Data"
      ],
      "metadata": {
        "id": "fYPpYVlJ7ALs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#downloading preprocessed data\n",
        "!gdown --id 12rNfOiM4allOn8t4l0ErpvgmA9hEZPh2 -O processed_labels.zip\n",
        "!gdown --id 145Ul0rdbQcX98lQz8IO5eOtI0ZlvKwz6 -O processed_images.zip"
      ],
      "metadata": {
        "id": "nlkvBNXItiHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#unzipping the preprocessed data to local dir\n",
        "!unzip /content/processed_images.zip\n",
        "!unzip /content/processed_labels.zip"
      ],
      "metadata": {
        "id": "iRtqOCbcvSTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-Processing Data Step 1"
      ],
      "metadata": {
        "id": "tCDqzox07E_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path to the directory for both image and label data \n",
        "img_path = '/content/content/processed_images/*'\n",
        "lab_path = '/content/content/processed_labels/*'\n",
        "#one sample data point\n",
        "#/content/content/processed_images/aachen_000000_000019.png\n",
        "#/content/content/processed_labels/aachen_000000_000019.png"
      ],
      "metadata": {
        "id": "AgJMIX2_dDHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the data indo a list, numpy array\n",
        "%%time\n",
        "train_images = glob.glob(img_path)\n",
        "train_labels = glob.glob(lab_path)\n",
        "print(len(train_images),len(train_labels))\n"
      ],
      "metadata": {
        "id": "4ZMBHtR1veez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=40\n",
        "m=2310"
      ],
      "metadata": {
        "id": "9JKgRGjYrjCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.array(x_train['gray'].values)\n",
        "t = t.reshape(int(t.shape[0]/40000),40000)\n",
        "display_img(t[0].reshape(200,200))"
      ],
      "metadata": {
        "id": "FdtUv0ZEwS1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test and train images \n",
        "x_train = create_train_image_df(train_images[:n])\n",
        "x_test = create_test_image_df(train_images[2300:m])\n",
        "print(x_train.shape,x_test.shape )\n",
        "x_train.head()"
      ],
      "metadata": {
        "id": "J1yBL0DknMZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.head(2)"
      ],
      "metadata": {
        "id": "sW4cyfqJnMec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating train mask image Data Frame\n",
        "%%time\n",
        "dim = (200,200)\n",
        "y_train = pd.DataFrame()\n",
        "\n",
        "for label_dir in train_labels[:n]:\n",
        "  df2 = pd.DataFrame()\n",
        "  lab = cv2.imread(label_dir, 0)\n",
        "  #lab = cv2.resize(lab, dim).reshape(-1)\n",
        "  df2['label'] = cv2.resize(lab, dim).reshape(-1) \n",
        "  y_train = pd.concat([y_train, df2])\n",
        "print(y_train.shape)\n",
        "y_train.head(2)"
      ],
      "metadata": {
        "id": "FJeuq2KZrU4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating test mask image Data Frame\n",
        "%%time\n",
        "dim = (200,200)\n",
        "y_test = pd.DataFrame()\n",
        "\n",
        "for label_dir in train_labels[2300:m]:\n",
        "  df2 = pd.DataFrame()\n",
        "  lab = cv2.imread(label_dir, 0)\n",
        "  #lab = cv2.resize(lab, dim).reshape(-1)\n",
        "  df2['label'] = cv2.resize(lab, dim).reshape(-1) \n",
        "  y_test = pd.concat([y_test, df2])\n",
        "print(y_test.shape)\n",
        "y_test.head(2)"
      ],
      "metadata": {
        "id": "_67502CwqnKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.concat([x_train, y_train], axis=1)\n",
        "print(df_train.shape)\n",
        "df_train.head(2)"
      ],
      "metadata": {
        "id": "gsuVHFS2qnPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.concat([x_test, y_test], axis=1)\n",
        "print(df_test.shape)\n",
        "df_test.head(2)"
      ],
      "metadata": {
        "id": "C5eM82vR1Hnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.shape)\n",
        "print(df_test.shape)"
      ],
      "metadata": {
        "id": "APQbBEFg1aRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/DL_Project"
      ],
      "metadata": {
        "id": "dPHqaN4g8job"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#only execute this cell to save the preprocessed dataframe to google drive \n",
        "df_train.to_csv('df_train.csv', index=False)\n",
        "df_test.to_csv('df_test.csv', index=False)\n",
        "#Changing directory back to \"Content\"\n"
      ],
      "metadata": {
        "id": "x8SZLWEi7If3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "WRuWlUbn8tcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PreProcess Data Step 2"
      ],
      "metadata": {
        "id": "TKTRjTQq_R6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(df_train.shape)\n",
        "#print(df_test.shape)\n",
        "print(df_train.shape, df_test.shape)"
      ],
      "metadata": {
        "id": "JjO1GlhE1fq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this cell only if you want to use GPU and GPU is setup\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "pVkRepNvCu3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this cell only if you want to remove all zero value where label = 0. \n",
        "df_train = df_train[df_train.label != 0]\n",
        "df_test = df_test[df_test.label != 0]\n",
        "print(df_train.shape, df_test.shape)"
      ],
      "metadata": {
        "id": "tc6AFIEj1f0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train = df_train.drop(labels = [\"label\"], axis=1)\n",
        "Y_Train = df_train[\"label\"].values\n",
        "\n",
        "X_Test = df_test.drop(labels = [\"label\"], axis=1)\n",
        "Y_Test = df_test[\"label\"].values\n",
        "\n",
        "\n",
        "print((X_Train.shape, Y_Train.shape),(X_Test.shape, Y_Test.shape))"
      ],
      "metadata": {
        "id": "T177MN4P2eGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Train = LabelEncoder().fit_transform(Y_Train)\n",
        "Y_Test = LabelEncoder().fit_transform(Y_Test)\n",
        "print(Y_Train.shape, Y_Test.shape)"
      ],
      "metadata": {
        "id": "8FEgcJoo2ePK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ],
      "metadata": {
        "id": "YYZ0a30I6NNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None, \n",
        "                            min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
        "                            max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
        "                            bootstrap=True, oob_score=False, n_jobs=None, random_state=2, \n",
        "                            verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, \n",
        "                            max_samples=None)"
      ],
      "metadata": {
        "id": "WIIIcyUUzTY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FIT & Save Model"
      ],
      "metadata": {
        "id": "gB-kE0C26Zqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/DL_Project"
      ],
      "metadata": {
        "id": "0bhiHlMjqT3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## Train the model on training data\n",
        "model.fit(X_Train, Y_Train)"
      ],
      "metadata": {
        "id": "il3_TV0q29MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the trained model as pickle string to google drive for future use \n",
        "filename = \"rf_model2_withzero.sav\"\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "#Changing directory back to \"Content\"\n",
        "cd /content"
      ],
      "metadata": {
        "id": "euEnyJ0KH8hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "bhz6ChqizydG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_test = model.predict(X_Test)"
      ],
      "metadata": {
        "id": "0auJH93AgVHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Testing Accuracy = \", metrics.accuracy_score(Y_Test, prediction_test))"
      ],
      "metadata": {
        "id": "vt7LxLRO0jJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = prediction_test.reshape(int(prediction_test.shape[0]/40000),40000)\n",
        "Y_true = Y_Test.reshape(int(Y_Test.shape[0]/40000),40000)\n",
        "print(Y_true.shape,Y_pred.shape )"
      ],
      "metadata": {
        "id": "xSsXPBvc1XjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Testing Accuracy = \", metrics.accuracy_score(Y_true[0].reshape(200,200), Y_pred[0].reshape(200,200)))"
      ],
      "metadata": {
        "id": "wcVE4OgS6EEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import jaccard_score"
      ],
      "metadata": {
        "id": "I8-O9m_zjOEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Training Accuracy = \", jaccard_score(Y_true, Y_pred,average='micro'))"
      ],
      "metadata": {
        "id": "_cF0vQENjQ28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3lovB0fgt-5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "WoXAEEd0sJk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gabor_feature_extraction(img, lamda, gamma):\n",
        "  kernel = cv2.getGaborKernel((8, 8), 1, 0, lamda, gamma, 0, ktype=cv2.CV_32F)\n",
        "  gabor_img = cv2.filter2D(img, cv2.CV_8UC3, kernel)\n",
        "  return gabor_img.reshape(-1,1)/255\n"
      ],
      "metadata": {
        "id": "-vv5vreOsLXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(img):\n",
        "  #function will accept a numpy.ndarray\n",
        "\n",
        "  #GAUSSIAN blur 1\n",
        "  gblur = cv2.GaussianBlur(img, (5,5),7)\n",
        "  #Laplacian\n",
        "  laplacian = cv2.Laplacian(img, cv2.CV_64F, ksize=7) \n",
        "  #SOBEL\n",
        "  sobele = cv2.Sobel(img,cv2.CV_64F, 0,1,ksize=5)\n",
        "  #CANNY \n",
        "  cany = cv2.Canny(np.uint8(sobele), 100,200)\n",
        "\n",
        "  #return cany\n",
        "  return (gblur.reshape(-1)/255,laplacian.reshape(-1)/255,sobele.reshape(-1)/255,cany.reshape(-1)/255)"
      ],
      "metadata": {
        "id": "CZ_JrjPPsLz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image_org(img):\n",
        "  #GAUSSIAN blur 1\n",
        "  blur1 = nd.gaussian_filter(img, sigma=3)\n",
        "  #GAUSSIAN blur2\n",
        "  blur2 = nd.gaussian_filter(img, sigma=7)\n",
        "  #SOBEL\n",
        "  sobele = sobel(blur2)\n",
        "  #CANNY \n",
        "  cany = cv2.Canny(np.uint8(sobele), 100,200)\n",
        "  #return cany\n",
        "  return (blur1.reshape(-1,1),blur2.reshape(-1,1),sobele.reshape(-1,1),cany.reshape(-1,1))"
      ],
      "metadata": {
        "id": "BVCA-uLQsL2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_img(img):\n",
        "  fig = plt.figure(figsize=(8,8))\n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.imshow(img,cmap='gray')"
      ],
      "metadata": {
        "id": "00mkCqMssL5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_img(image_path):\n",
        "  return cv2.imread(image_path, 0).astype(np.float32)/255\n"
      ],
      "metadata": {
        "id": "1ocjCADFplBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_one_test_image(image_dir):\n",
        "  img = cv2.imread(image_dir, 0)\n",
        "  g1 = img/255\n",
        "  _,thresh3  = cv2.threshold(img, 80, 255, cv2.THRESH_BINARY_INV)\n",
        "  binay_inv = thresh3/255\n",
        "  adaptive_thresh = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11, 11)/255\n",
        "  gb1 = gabor_feature_extraction(img,.8, .05 )/255\n",
        "  gb2 = gabor_feature_extraction(img,1.6, .5 )/255\n",
        "  blur, laplacian, sobel, cany = preprocess_image(img)\n",
        "  return np.hstack((g1.reshape(-1,1), binay_inv.reshape(-1,1), adaptive_thresh.reshape(-1,1), \n",
        "                    gb1.reshape(-1,1), gb2.reshape(-1,1), blur.reshape(-1,1), \n",
        "                    laplacian.reshape(-1,1), sobel.reshape(-1,1), cany.reshape(-1,1)))\n"
      ],
      "metadata": {
        "id": "1zQxJi2WplEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_one_test_image_df(image_dir):\n",
        "  dim = (200,200)\n",
        "  test_df = pd.DataFrame()\n",
        "  img = cv2.imread(image_dir, 0)\n",
        "  img = cv2.resize(img, dim)\n",
        "  g1 = img/255\n",
        "  _,thresh3  = cv2.threshold(img, 80, 255, cv2.THRESH_BINARY_INV)\n",
        "  binay_inv = thresh3/255\n",
        "  adaptive_thresh = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11, 11)/255\n",
        "  gb1 = gabor_feature_extraction(img,.8, .05 )/255\n",
        "  gb2 = gabor_feature_extraction(img,1.6, .5 )/255\n",
        "  blur, laplacian, sobel, cany = preprocess_image(img)\n",
        "\n",
        "  test_df['gray']= g1.reshape(-1)\n",
        "  test_df['binay_inv']= binay_inv.reshape(-1)\n",
        "  test_df['adaptive_thresh']=adaptive_thresh.reshape(-1)\n",
        "  test_df['gabor1'] = gabor_feature_extraction(img,.8, .05 ).reshape(-1)\n",
        "  test_df['gabor2'] = gabor_feature_extraction(img,1.6, .5 ).reshape(-1)\n",
        "  test_df['blur'], test_df['laplacian'], test_df['sobelx'], test_df['canny'] = blur.reshape(-1), laplacian.reshape(-1), sobel.reshape(-1), cany.reshape(-1)\n",
        "\n",
        "  return test_df\n"
      ],
      "metadata": {
        "id": "rx5nfg_-plH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_one_test_image_df_a(image_dir):\n",
        "  #this function is getting an image as an input \n",
        "  test_df = pd.DataFrame()\n",
        "  #img = cv2.imread(image_dir, 0)\n",
        "  img = image_dir\n",
        "  g1 = img/255\n",
        "  _,thresh3  = cv2.threshold(img, 80, 255, cv2.THRESH_BINARY_INV)\n",
        "  binay_inv = thresh3/255\n",
        "  adaptive_thresh = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11, 11)/255\n",
        "  gb1 = gabor_feature_extraction(img,.8, .05 )/255\n",
        "  gb2 = gabor_feature_extraction(img,1.6, .5 )/255\n",
        "  blur, laplacian, sobel, cany = preprocess_image(img)\n",
        "\n",
        "  test_df['gray']= g1.reshape(-1)\n",
        "  test_df['binay_inv']= binay_inv.reshape(-1)\n",
        "  test_df['adaptive_thresh']=adaptive_thresh.reshape(-1)\n",
        "  test_df['gabor1'] = gabor_feature_extraction(img,.8, .05 ).reshape(-1)\n",
        "  test_df['gabor2'] = gabor_feature_extraction(img,1.6, .5 ).reshape(-1)\n",
        "  test_df['blur'], test_df['laplacian'], test_df['sobelx'], test_df['canny'] = blur.reshape(-1), laplacian.reshape(-1), sobel.reshape(-1), cany.reshape(-1)\n",
        "\n",
        "  return test_df"
      ],
      "metadata": {
        "id": "qvBiGJ_nek2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_images(img, gt, pred):\n",
        "    if pred is not None:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(12, 8))\n",
        "    else:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(8, 8))\n",
        "\n",
        "    axes[0].imshow(img)\n",
        "    axes[0].set_title('Actual Image')\n",
        "\n",
        "    axes[1].imshow(gt)\n",
        "    axes[1].set_title('Masked Image')\n",
        "    \n",
        "    if pred is not None:\n",
        "        axes[2].imshow(pred)\n",
        "        axes[2].set_title('Predicted Image')"
      ],
      "metadata": {
        "id": "L5OnUvljBEEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(X,filters,block):\n",
        "    # resiudal block with dilated convolutions\n",
        "    # add skip connection at last after doing convoluion\n",
        "\n",
        "    b = 'block_'+str(block)+'_'\n",
        "    f1,f2,f3 = filters\n",
        "    X_skip = X\n",
        "\n",
        "    # block_a\n",
        "    X = Conv2D(filters=f1,kernel_size=(1,1),dilation_rate=(1,1),\n",
        "               padding='same',kernel_initializer='he_normal',name=b+'a')(X)\n",
        "    X = BatchNormalization(name=b+'batch_norm_a')(X)\n",
        "    X = LeakyReLU(alpha=0.2,name=b+'leakyrelu_a')(X)\n",
        "    # block_b\n",
        "    X = Conv2D(filters=f2,kernel_size=(3,3),dilation_rate=(2,2),\n",
        "               padding='same',kernel_initializer='he_normal',name=b+'b')(X)\n",
        "    X = BatchNormalization(name=b+'batch_norm_b')(X)\n",
        "    X = LeakyReLU(alpha=0.2,name=b+'leakyrelu_b')(X)\n",
        "    # block_c\n",
        "    X = Conv2D(filters=f3,kernel_size=(1,1),dilation_rate=(1,1),\n",
        "               padding='same',kernel_initializer='he_normal',name=b+'c')(X)\n",
        "    X = BatchNormalization(name=b+'batch_norm_c')(X)\n",
        "    # skip_conv\n",
        "    X_skip = Conv2D(filters=f3,kernel_size=(3,3),padding='same',name=b+'skip_conv')(X_skip)\n",
        "    X_skip = BatchNormalization(name=b+'batch_norm_skip_conv')(X_skip)\n",
        "    # block_c + skip_conv\n",
        "    X = Add(name=b+'add')([X,X_skip])\n",
        "    X = ReLU(name=b+'relu')(X)\n",
        "    return X\n",
        "    \n",
        "def base_feature_maps(input_layer):\n",
        "    # base covolution module to get input image feature maps \n",
        "    \n",
        "    # block_1\n",
        "    base = conv_block(input_layer,[16,16,32],'1')\n",
        "    # block_2\n",
        "    base = conv_block(base,[16,16,32],'2')\n",
        "    return base\n",
        "\n",
        "def pyramid_feature_maps(input_layer):\n",
        "    # pyramid pooling module\n",
        "    \n",
        "    base = base_feature_maps(input_layer)\n",
        "    # red\n",
        "    red = GlobalAveragePooling2D(name='red_pool')(base)\n",
        "    red = tf.keras.layers.Reshape((1,1,32))(red)\n",
        "    red = Conv2D(filters=32,kernel_size=(1,1),name='red_1_by_1')(red)\n",
        "    red = UpSampling2D(size=128,interpolation='bilinear',name='red_upsampling')(red)\n",
        "    red = tf.image.resize(red, [IMG_SIZE, IMG_SIZE])\n",
        "    # yellow\n",
        "    yellow = AveragePooling2D(pool_size=(2,2),name='yellow_pool')(base)\n",
        "    yellow = Conv2D(filters=32,kernel_size=(1,1),name='yellow_1_by_1')(yellow)\n",
        "    yellow = UpSampling2D(size=2,interpolation='bilinear',name='yellow_upsampling')(yellow)\n",
        "    yellow = tf.image.resize(yellow, [IMG_SIZE, IMG_SIZE])\n",
        "    # blue\n",
        "    blue = AveragePooling2D(pool_size=(4,4),name='blue_pool')(base)\n",
        "    blue = Conv2D(filters=32,kernel_size=(1,1),name='blue_1_by_1')(blue)\n",
        "    blue = UpSampling2D(size=4,interpolation='bilinear',name='blue_upsampling')(blue)\n",
        "    blue = tf.image.resize(blue, [IMG_SIZE, IMG_SIZE])\n",
        "    # green\n",
        "    green = AveragePooling2D(pool_size=(8,8),name='green_pool')(base)\n",
        "    green = Conv2D(filters=32,kernel_size=(1,1),name='green_1_by_1')(green)\n",
        "    green = UpSampling2D(size=8,interpolation='bilinear',name='green_upsampling')(green)\n",
        "    green = tf.image.resize(green, [IMG_SIZE, IMG_SIZE])\n",
        "    # base + red + yellow + blue + green\n",
        "    return tf.keras.layers.concatenate([base,red,yellow,blue,green])\n",
        "\n",
        "def last_conv_module(input_layer):\n",
        "    X = pyramid_feature_maps(input_layer)\n",
        "    X = Conv2D(filters=3,kernel_size=3,padding='same',name='last_conv_3_by_3')(X)\n",
        "    X = BatchNormalization(name='last_conv_3_by_3_batch_norm')(X)\n",
        "    X = Activation('sigmoid',name='last_conv_relu')(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "LnYFbrjZBM02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(history):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(1,len(acc)+1)\n",
        "\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.plot(epochs, acc, color='blue', label='Train')\n",
        "  plt.plot(epochs, val_acc, color='orange', label='Val')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  _ = plt.figure()\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.plot(epochs, loss, color='blue', label='Train')\n",
        "  plt.plot(epochs, val_loss, color='orange', label='Val')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "h3aNpbOIBZ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_image_df(train_images,n):\n",
        "  '''provide a list of image directory and n= number of image'''\n",
        "  image_df = pd.DataFrame()\n",
        "  for img_dir in train_images[:n]:\n",
        "    image_df = pd.concat([image_df, generate_one_test_image_df(img_dir)])\n",
        "  return image_df"
      ],
      "metadata": {
        "id": "M8iJi35OBZ7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_image_df(train_images):\n",
        "  '''provide a list of image directory and n= number of image you want , test image starts from 2300'''\n",
        "  image_df = pd.DataFrame()\n",
        "  for img_dir in train_images:\n",
        "    image_df = pd.concat([image_df, generate_one_test_image_df(img_dir)])\n",
        "  print('train', image_df.shape)\n",
        "  return image_df\n",
        "\n",
        "def create_test_image_df(train_images):\n",
        "  import pandas as pd\n",
        "  '''provide a list of image directory and n= number of image'''\n",
        "  image_df = pd.DataFrame()\n",
        "  for img_dir in train_images:\n",
        "    single_img_df = generate_one_test_image_df(img_dir)\n",
        "    image_df = pd.concat([image_df,single_img_df ])\n",
        "  print('test', image_df.shape)\n",
        "  return image_df"
      ],
      "metadata": {
        "id": "P_SA7aY0jisz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_data(df_train, df_test):\n",
        "  '''df_train, df_test > provide both test and train df with image data and label data'''\n",
        "  df_train = pd.read_csv(df_train) \n",
        "  df_test = pd.read_csv(df_test) \n",
        "  X_Train = df_train.drop(labels = [\"label\"], axis=1)\n",
        "  Y_Train = df_train[\"label\"].values\n",
        "\n",
        "  X_Test = df_test.drop(labels = [\"label\"], axis=1)\n",
        "  Y_Test = df_test[\"label\"].values\n",
        "\n",
        "  Y_Train = LabelEncoder().fit_transform(Y_Train)\n",
        "  Y_Test = LabelEncoder().fit_transform(Y_Test)\n",
        "\n",
        "  return X_Train,X_Test, Y_Train, Y_Test"
      ],
      "metadata": {
        "id": "Bd90XMhevRZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rf_accuracy(model, X, Y):\n",
        "  'provide the randomforest model, X: test/train data, y: test/train label '\n",
        "  prediction_test = model.predict(X)\n",
        "  Y_pred = prediction_test.reshape(int(prediction_test.shape[0]/40000),40000)\n",
        "  Y_true = Y.reshape(int(Y.shape[0]/40000),40000)\n",
        "  return metrics.accuracy_score(Y_true[0].reshape(200,200), Y_pred[0].reshape(200,200))"
      ],
      "metadata": {
        "id": "3l-k6ezk9Hyn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}