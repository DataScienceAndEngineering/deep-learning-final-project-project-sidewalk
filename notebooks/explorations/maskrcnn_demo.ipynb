{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/processed')\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "# LABELS_DIR = os.path.join(ROOT_DIR, 'data/interim')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "# from IPython.display import display\n",
    "import skimage\n",
    "from skimage.draw import polygon\n",
    "import json \n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.maskrcnn.mrcnn import visualize\n",
    "from models.maskrcnn.mrcnn import model\n",
    "# from models.maskrcnn.mrcnn import log\n",
    "from models.maskrcnn.mrcnn import config\n",
    "from models.maskrcnn.mrcnn.config import Config\n",
    "from models.maskrcnn.mrcnn import model as modellib, utils\n",
    "\n",
    "from src.data import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR,'models/maskrcnn/mask_rcnn_coco.h5')\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConfig(Config):\n",
    "  NAME = 'sidewalks'\n",
    "  GPU_COUNT = 1\n",
    "\n",
    "  # 12GB GPU can typical handle 2 images (1024 x 1024)\n",
    "  # ColabPro High Ram has roughly 27GB GPU\n",
    "  IMAGES_PER_GPU = 4\n",
    "\n",
    "  # Background + 1 sidewalk class \n",
    "  NUM_CLASSES = 1 + 10\n",
    "\n",
    "  # Number of training steps per epoch\n",
    "  STEPS_PER_EPOCH = 10\n",
    "\n",
    "  # Number of validation steps to runa tt eh end of \n",
    "  # every epoch,larger number improves better accuracy\n",
    "  # but slows down training\n",
    "  VALIDATION_STEPS = 50\n",
    "\n",
    "  # ROIs belwo this threshold are skipped\n",
    "  DETECTION_MIN_CONFIDENCE = .7   \n",
    "  DETECTION_NMS_THRESHOLD = 0.3\n",
    "  LEARNING_RATE =0.0005\n",
    "  BACKBONE = 'resnet101'\n",
    "  IMAGE_MAX_DIM = 512\n",
    "  IMAGE_MIN_DIM = 256\n",
    "  \n",
    "class CustomDataset(utils.Dataset):\n",
    "  def load_dataset(self,dataset_dir,subset):\n",
    "    # self.image_info = {}\n",
    "    # self.class_info = {}\n",
    "    # self.data_directory = dataset_dir\n",
    "\n",
    "    classes = pd.read_csv(os.path.join(dataset_dir,'classes.csv'))\n",
    "    classes = classes.set_index('CLASS_ID').T.to_dict('list')\n",
    "    classes = dict([(k,classes[k][0]) for k,v in classes.items()])\n",
    "\n",
    "    labels = pd.read_csv(os.path.join(dataset_dir, 'labels.csv'), index_col=None)\n",
    "    labels = labels.set_index('ID').T.to_dict('list')\n",
    "    for id in labels:\n",
    "      labs = labels[id][0]\n",
    "      labels[id] =  np.array([int(i) for i in labs.split(' ')[:-1]])\n",
    "    \n",
    "\n",
    "    for class_id,class_name in classes.items():\n",
    "      self.add_class(source='cityscape',\n",
    "                   class_id = class_id,\n",
    "                   class_name = class_name)\n",
    "      \n",
    "    # self.class_names = [d['name'] for d in self.class_info.values()]\n",
    "    \n",
    "    # iterating to get the image ids\n",
    "    if subset == 'validation':\n",
    "      subset = 'val'\n",
    "\n",
    "    SUBSET_IDs = pd.read_csv(os.path.join(dataset_dir, subset+'.csv'), index_col=0) \n",
    "    SUBSET_IDs = [i.split('.')[0] for i in SUBSET_IDs['0'].tolist()]\n",
    "\n",
    "    for ID in SUBSET_IDs:\n",
    "    # for i in range(len(data)):\n",
    "      img_path = os.path.join(dataset_dir, f'images/{ID}.png')\n",
    "      img_labels = labels[ID]\n",
    "      self.add_image(source='cityscape',\n",
    "                     image_id = ID,\n",
    "                     path = img_path,\n",
    "                     labels = img_labels)\n",
    "      \n",
    "  def load_mask(self,id):\n",
    "    # if not isinstance(id, str):\n",
    "    #   id = self.image_info[id]\n",
    "\n",
    "    img_info = self.image_info[id]\n",
    "    img_name = img_info['id']\n",
    "    mask_path = os.path.join(os.path.dirname(os.path.dirname(img_info['path']))\n",
    "                        ,'segmentations',f'{img_name}.json')\n",
    "\n",
    "    masks = helper.create_mask(mask_path, img_info['labels'], resize=(256,512))\n",
    "    return masks, img_info['labels']\n",
    "  \n",
    "  def image_reference(self, id):\n",
    "    info = self.image_info[id]\n",
    "    if info[\"source\"] == \"cityscape\":\n",
    "        return info[\"id\"]\n",
    "    else:\n",
    "        super(self.__class__, self).image_reference(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CustomConfig()\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataset.load_dataset(DATA_DIR,'train')\n",
    "dataset.prepare()\n",
    "\n",
    "val_set = CustomDataset()\n",
    "val_set.load_dataset(DATA_DIR,'val')\n",
    "val_set.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0\n",
    "img = dataset.load_image(id)\n",
    "masks,labels = dataset.load_mask(id)\n",
    "names = dataset.class_from_source_map\n",
    "\n",
    "visualize.display_top_masks(img,\n",
    "                            masks,\n",
    "                            labels,\n",
    "                            names,\n",
    "                            limit=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "temp_log = os.path.join(ROOT_DIR,'data/external')\n",
    "model = modellib.MaskRCNN(mode='training',config=config, model_dir=temp_log)\n",
    "model_path = os.path.join(ROOT_DIR,'models/maskrcnn/mask_rcnn_coco.h5')\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(dataset, val_set, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.engine.training_v1 as t\n",
    "import keras.engine.training_utils_v1 as t_u\n",
    "import keras.backend as backend\n",
    "\n",
    "# debuging ValueError: Error when checking input: \n",
    "# expected input_image_meta to have shape (15,) \n",
    "# but got array with shape (13,)\n",
    "\n",
    "model_inputs = t_u.ModelInputs(dataset)\n",
    "inputs = model_inputs.get_symbolic_inputs(return_single_as_list=True)\n",
    "input_names = model_inputs.get_input_names()\n",
    "\n",
    "\n",
    "f_inputs = []\n",
    "f_input_names = []\n",
    "f_input_shapes = []\n",
    "\n",
    "for k, v in model_inputs.as_dict():\n",
    "    print(k,v)\n",
    "    \n",
    "    f_input_names.append(k)\n",
    "    f_inputs.append(v)\n",
    "    f_input_shapes.append(backend.int_shape(v))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs.as_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(ROOT_DIR,'models/maskrcnn/mask_rcnn_coco.h5')\n",
    "\n",
    "class InferenceConfig(CustomConfig):\n",
    "  GPU_COUNT =1 \n",
    "  IMAGES_PER_GPU = 1\n",
    "  DETECTION_MIN_CONFIDENCE = .5\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "temp_log = os.path.join(ROOT_DIR,'data/external')\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = modellib.MaskRCNN(mode = 'inference',\n",
    "                          config = inference_config,\n",
    "                          model_dir=temp_log)\n",
    "model.load_weights(model_path, by_name=True, \n",
    "                   exclude=['mrcnn_class_logits','mrcnn_bbox_fc','mrcnn_bbox','mrcnn_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([dataset.load_image('jena_000048_000019')],verbose=1)\n",
    "\n",
    "# display preferences\n",
    "def get_ax(rows=1, cols=1, size=10):\n",
    "  _,ax = plt.subplots(rows,cols,figsize=(size*cols,size*rows))\n",
    "  return ax\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(dataset.load_image('jena_000048_000019'), r['rois'], r['masks'], r['class_ids'], \n",
    "                            ['BG','sidewalk'], r['scores'], ax=get_ax(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(dataset.load_image('jena_000048_000019'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = dataset.load_mask('jena_000048_000019')\n",
    "c = dataset.load_image('jena_000048_000019')\n",
    "\n",
    "visualize.display_top_masks(c,a,b,dataset.class_info,limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = utils.extract_bboxes(a)\n",
    "print(bb.shape)\n",
    "visualize.display_instances(c, bb, a, np.array(b), dataset.class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF1 -> TF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as KL\n",
    "import keras.engine as KE\n",
    "import tensorflow.keras.models as KM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## norm_boxes_graph\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = KL.Input(shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\")\n",
    "input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE],\n",
    "                            name=\"input_image_meta\")\n",
    "\n",
    "# RPN GT\n",
    "input_rpn_match = KL.Input(\n",
    "                shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32)\n",
    "input_rpn_bbox = KL.Input(\n",
    "                shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32)\n",
    "\n",
    "# Detection GT (class IDs, bounding boxes, and masks)\n",
    "            # 1. GT Class IDs (zero padded)\n",
    "input_gt_class_ids = KL.Input(\n",
    "                            shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32)\n",
    "# 2. GT Boxes in pixels (zero padded)\n",
    "# [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates\n",
    "input_gt_boxes = KL.Input(\n",
    "                            shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_boxes_graph(boxes, shape):\n",
    "    \"\"\"Converts boxes from pixel coordinates to normalized coordinates.\n",
    "    boxes: [..., (y1, x1, y2, x2)] in pixel coordinates\n",
    "    shape: [..., (height, width)] in pixels\n",
    "\n",
    "    Note: In pixel coordinates (y2, x2) is outside the box. But in normalized\n",
    "    coordinates it's inside the box.\n",
    "\n",
    "    Returns:\n",
    "        [..., (y1, x1, y2, x2)] in normalized coordinates\n",
    "    \"\"\"\n",
    "    h, w = tf.split(tf.cast(shape, tf.float32), 2)\n",
    "    print(h,w)\n",
    "    scale = tf.concat([h, w, h, w], axis=-1) - tf.constant(1.0)\n",
    "    shift = tf.constant([0., 0., 1., 1.])\n",
    "    return tf.divide(boxes - shift, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL.Lambda(lambda x: norm_boxes_graph(\n",
    "                x,K.shape(input_image)[1:3]))(input_gt_boxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test/trials pls ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mask, _ = dataset.load_mask(0)\n",
    "plt.imshow(sample_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.image_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bboxes(mask):\n",
    "    \"\"\"Compute bounding boxes from masks.\n",
    "    mask: [height, width, num_instances]. Mask pixels are either 1 or 0.\n",
    "\n",
    "    Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n",
    "    \"\"\"\n",
    "    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n",
    "    for i in range(mask.shape[-1]):\n",
    "        m = mask[:, :, i]\n",
    "        # Bounding box.\n",
    "        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n",
    "        vertical_indicies = np.where(np.any(m, axis=1))[0]\n",
    "        if horizontal_indicies.shape[0]:\n",
    "            x1, x2 = horizontal_indicies[[0, -1]]\n",
    "            y1, y2 = vertical_indicies[[0, -1]]\n",
    "            # x2 and y2 should not be part of the box. Increment by 1.\n",
    "            x2 += 1\n",
    "            y2 += 1\n",
    "        else:\n",
    "            # No mask for this instance. Might happen due to\n",
    "            # resizing or cropping. Set bbox to zeros\n",
    "            x1, x2, y1, y2 = 0, 0, 0, 0\n",
    "        boxes[i] = np.array([y1, x1, y2, x2])\n",
    "    return boxes.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = extract_bboxes(sample_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x==8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = skimage.io.imread('/Users/csea/Desktop/deep-learning-final-project-project-sidewalk/data/interim/ZIP.gtFine_trainvaltest.zip/gtFine/train/zurich/zurich_000069_000019_gtFine_instanceIds.png')\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x==8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_bboxes(x==8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.png"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaskRCNN() -> build() -> DetectionTargetLayer() Stuck here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CustomConfig()\n",
    "config.display()\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataset.load_dataset(DATA_DIR,'train')\n",
    "dataset.prepare()\n",
    "\n",
    "val_set = CustomDataset()\n",
    "val_set.load_dataset(DATA_DIR,'val')\n",
    "val_set.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_log = os.path.join(ROOT_DIR,'data/external')\n",
    "# model = modellib.MaskRCNN(mode='training',config=config, model_dir=temp_log)\n",
    "\n",
    "mode = 'training'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_targets_graph(proposals, gt_class_ids, gt_boxes, gt_masks, config):\n",
    "    \"\"\"Generates detection targets for one image. Subsamples proposals and\n",
    "    generates target class IDs, bounding box deltas, and masks for each.\n",
    "\n",
    "    Inputs:\n",
    "    proposals: [POST_NMS_ROIS_TRAINING, (y1, x1, y2, x2)] in normalized coordinates. Might\n",
    "               be zero padded if there are not enough proposals.\n",
    "    gt_class_ids: [MAX_GT_INSTANCES] int class IDs\n",
    "    gt_boxes: [MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized coordinates.\n",
    "    gt_masks: [height, width, MAX_GT_INSTANCES] of boolean type.\n",
    "\n",
    "    Returns: Target ROIs and corresponding class IDs, bounding box shifts,\n",
    "    and masks.\n",
    "    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized coordinates\n",
    "    class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs. Zero padded.\n",
    "    deltas: [TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw))]\n",
    "    masks: [TRAIN_ROIS_PER_IMAGE, height, width]. Masks cropped to bbox\n",
    "           boundaries and resized to neural network output size.\n",
    "\n",
    "    Note: Returned arrays might be zero padded if not enough target ROIs.\n",
    "    \"\"\"\n",
    "    # Assertions\n",
    "    asserts = [\n",
    "        tf.Assert(tf.greater(tf.shape(input=proposals)[0], 0), [proposals],\n",
    "                  name=\"roi_assertion\"),\n",
    "    ]\n",
    "    with tf.control_dependencies(asserts):\n",
    "        proposals = tf.identity(proposals)\n",
    "\n",
    "    # Remove zero padding\n",
    "    proposals, _ = modellib.trim_zeros_graph(proposals, name=\"trim_proposals\")\n",
    "    gt_boxes, non_zeros = modellib.trim_zeros_graph(gt_boxes, name=\"trim_gt_boxes\")\n",
    "    gt_class_ids = tf.boolean_mask(tensor=gt_class_ids, mask=non_zeros,\n",
    "                                   name=\"trim_gt_class_ids\")\n",
    "    gt_masks = tf.gather(gt_masks, tf.compat.v1.where(non_zeros)[:, 0], axis=2,\n",
    "                         name=\"trim_gt_masks\")\n",
    "\n",
    "    # Handle COCO crowds\n",
    "    # A crowd box in COCO is a bounding box around several instances. Exclude\n",
    "    # them from training. A crowd box is given a negative class ID.\n",
    "    crowd_ix = tf.compat.v1.where(gt_class_ids < 0)[:, 0]\n",
    "    non_crowd_ix = tf.compat.v1.where(gt_class_ids > 0)[:, 0]\n",
    "    crowd_boxes = tf.gather(gt_boxes, crowd_ix)\n",
    "    gt_class_ids = tf.gather(gt_class_ids, non_crowd_ix)\n",
    "    gt_boxes = tf.gather(gt_boxes, non_crowd_ix)\n",
    "    gt_masks = tf.gather(gt_masks, non_crowd_ix, axis=2)\n",
    "\n",
    "    # Compute overlaps matrix [proposals, gt_boxes]\n",
    "    overlaps = modellib.overlaps_graph(proposals, gt_boxes)\n",
    "    \n",
    "\n",
    "    # Compute overlaps with crowd boxes [proposals, crowd_boxes]\n",
    "    crowd_overlaps = modellib.overlaps_graph(proposals, crowd_boxes)\n",
    "    crowd_iou_max = tf.reduce_max(input_tensor=crowd_overlaps, axis=1)\n",
    "    no_crowd_bool = (crowd_iou_max < 0.001)\n",
    "\n",
    "    # Determine positive and negative ROIs\n",
    "    roi_iou_max = tf.reduce_max(input_tensor=overlaps, axis=1)\n",
    "    # 1. Positive ROIs are those with >= 0.5 IoU with a GT box\n",
    "    positive_roi_bool = (roi_iou_max >= 0.5)\n",
    "    positive_indices = tf.compat.v1.where(positive_roi_bool)[:, 0]\n",
    "    # 2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.\n",
    "    negative_indices = tf.compat.v1.where(tf.logical_and(roi_iou_max < 0.5, no_crowd_bool))[:, 0]\n",
    "\n",
    "    # Subsample ROIs. Aim for 33% positive\n",
    "    # Positive ROIs\n",
    "    positive_count = int(config.TRAIN_ROIS_PER_IMAGE *\n",
    "                         config.ROI_POSITIVE_RATIO)\n",
    "    positive_indices = tf.random.shuffle(positive_indices)[:positive_count]\n",
    "    # print(f'positive indices {positive_indices.numpy()}')\n",
    "    positive_count = tf.shape(input=positive_indices)[0]\n",
    "    # Negative ROIs. Add enough to maintain positive:negative ratio.\n",
    "    r = 1.0 / config.ROI_POSITIVE_RATIO\n",
    "    negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count\n",
    "    negative_indices = tf.random.shuffle(negative_indices)[:negative_count]\n",
    "    # Gather selected ROIs\n",
    "    positive_rois = tf.gather(proposals, positive_indices)\n",
    "    negative_rois = tf.gather(proposals, negative_indices)\n",
    "\n",
    "    # Assign positive ROIs to GT boxes.\n",
    "    positive_overlaps = tf.gather(overlaps, positive_indices)\n",
    "    # print(f'positive overlaps {positive_overlaps}')\n",
    "    print('positive overlaps', positive_overlaps)\n",
    "\n",
    "##*****************************************************************\n",
    "    # tf cond does not work on kerasTensors \n",
    "        # so we create a wrapper around it \n",
    "    class ConditionalActivationLayer(KL.Layer):\n",
    "        def call(self, inputs):\n",
    "            return tf.cond(tf.greater(tf.shape(input=inputs)[1],0), \n",
    "                            lambda :tf.argmax(input=inputs,axis=1), \n",
    "                            lambda :tf.cast(tf.constant([]), tf.int64))\n",
    "        \n",
    "    # roi_gt_box_assignment = tf.cond(\n",
    "    #     pred=tf.greater(tf.shape(input=positive_overlaps)[1], 0),\n",
    "    #     true_fn=lambda: tf.argmax(input=positive_overlaps, axis=1),\n",
    "    #     false_fn=lambda: tf.cast(tf.constant([]), tf.int64)\n",
    "    # )\n",
    "\n",
    "    roi_gt_box_assignment = ConditionalActivationLayer()(positive_overlaps)\n",
    "\n",
    "    roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment)\n",
    "    roi_gt_class_ids = tf.gather(gt_class_ids, roi_gt_box_assignment)\n",
    "\n",
    "    # Compute bbox refinement for positive ROIs\n",
    "    deltas = utils.box_refinement_graph(positive_rois, roi_gt_boxes)\n",
    "    deltas /= config.BBOX_STD_DEV\n",
    "\n",
    "    # Assign positive ROIs to GT masks\n",
    "    # Permute masks to [N, height, width, 1]\n",
    "    transposed_masks = tf.expand_dims(tf.transpose(a=gt_masks, perm=[2, 0, 1]), -1)\n",
    "    # Pick the right mask for each ROI\n",
    "    roi_masks = tf.gather(transposed_masks, roi_gt_box_assignment)\n",
    "\n",
    "    # Compute mask targets\n",
    "    boxes = positive_rois\n",
    "    if config.USE_MINI_MASK:\n",
    "        # Transform ROI coordinates from normalized image space\n",
    "        # to normalized mini-mask space.\n",
    "        y1, x1, y2, x2 = tf.split(positive_rois, 4, axis=1)\n",
    "        gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(roi_gt_boxes, 4, axis=1)\n",
    "        gt_h = gt_y2 - gt_y1\n",
    "        gt_w = gt_x2 - gt_x1\n",
    "        y1 = (y1 - gt_y1) / gt_h\n",
    "        x1 = (x1 - gt_x1) / gt_w\n",
    "        y2 = (y2 - gt_y1) / gt_h\n",
    "        x2 = (x2 - gt_x1) / gt_w\n",
    "        boxes = tf.concat([y1, x1, y2, x2], 1)\n",
    "    box_ids = tf.range(0, tf.shape(input=roi_masks)[0])\n",
    "    masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes,\n",
    "                                     box_ids,\n",
    "                                     config.MASK_SHAPE)\n",
    "    # Remove the extra dimension from masks.\n",
    "    masks = tf.squeeze(masks, axis=3)\n",
    "\n",
    "    # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with\n",
    "    # binary cross entropy loss.\n",
    "    masks = tf.round(masks)\n",
    "\n",
    "    # Append negative ROIs and pad bbox deltas and masks that\n",
    "    # are not used for negative ROIs with zeros.\n",
    "    rois = tf.concat([positive_rois, negative_rois], axis=0)\n",
    "    N = tf.shape(input=negative_rois)[0]\n",
    "    P = tf.maximum(config.TRAIN_ROIS_PER_IMAGE - tf.shape(input=rois)[0], 0)\n",
    "    rois = tf.pad(tensor=rois, paddings=[(0, P), (0, 0)])\n",
    "    roi_gt_boxes = tf.pad(tensor=roi_gt_boxes, paddings=[(0, N + P), (0, 0)])\n",
    "    roi_gt_class_ids = tf.pad(tensor=roi_gt_class_ids, paddings=[(0, N + P)])\n",
    "    deltas = tf.pad(tensor=deltas, paddings=[(0, N + P), (0, 0)])\n",
    "    masks = tf.pad(tensor=masks, paddings=[[0, N + P], (0, 0), (0, 0)])\n",
    "\n",
    "    return rois, roi_gt_class_ids, deltas, masks\n",
    "\n",
    "def batch_slice(inputs, graph_fn, batch_size, names=None):\n",
    "    \"\"\"Splits inputs into slices and feeds each slice to a copy of the given\n",
    "    computation graph and then combines the results. It allows you to run a\n",
    "    graph on a batch of inputs even if the graph is written to support one\n",
    "    instance only.\n",
    "\n",
    "    inputs: list of tensors. All must have the same first dimension length\n",
    "    graph_fn: A function that returns a TF tensor that's part of a graph.\n",
    "    batch_size: number of slices to divide the data into.\n",
    "    names: If provided, assigns names to the resulting tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(inputs, list):\n",
    "        inputs = [inputs]\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(batch_size):\n",
    "        inputs_slice = [x[i] for x in inputs]\n",
    "        output_slice = graph_fn(*inputs_slice)\n",
    "        if not isinstance(output_slice, (tuple, list)):\n",
    "            output_slice = [output_slice]\n",
    "        outputs.append(output_slice)\n",
    "    # Change outputs from a list of slices where each is\n",
    "    # a list of outputs to a list of outputs and each has\n",
    "    # a list of slices\n",
    "    outputs = list(zip(*outputs))\n",
    "\n",
    "    if names is None:\n",
    "        names = [None] * len(outputs)\n",
    "\n",
    "    result = [tf.stack(o, axis=0, name=n)\n",
    "              for o, n in zip(outputs, names)]\n",
    "    if len(result) == 1:\n",
    "        result = result[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionTargetLayer(KL.Layer):\n",
    "    \"\"\"Subsamples proposals and generates target box refinement, class_ids,\n",
    "    and masks for each.\n",
    "\n",
    "    Inputs:\n",
    "    proposals: [batch, N, (y1, x1, y2, x2)] in normalized coordinates. Might\n",
    "               be zero padded if there are not enough proposals.\n",
    "    gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs.\n",
    "    gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized\n",
    "              coordinates.\n",
    "    gt_masks: [batch, height, width, MAX_GT_INSTANCES] of boolean type\n",
    "\n",
    "    Returns: Target ROIs and corresponding class IDs, bounding box shifts,\n",
    "    and masks.\n",
    "    rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized\n",
    "          coordinates\n",
    "    target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.\n",
    "    target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw)]\n",
    "    target_mask: [batch, TRAIN_ROIS_PER_IMAGE, height, width]\n",
    "                 Masks cropped to bbox boundaries and resized to neural\n",
    "                 network output size.\n",
    "\n",
    "    Note: Returned arrays might be zero padded if not enough target ROIs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(DetectionTargetLayer, self).__init__(**kwargs)\n",
    "        self.config = config\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DetectionTargetLayer, self).get_config()\n",
    "        config[\"config\"] = self.config.to_dict()\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        proposals = inputs[0]\n",
    "        gt_class_ids = inputs[1]\n",
    "        gt_boxes = inputs[2]\n",
    "        gt_masks = inputs[3]\n",
    "\n",
    "        # Slice the batch and run a graph for each slice\n",
    "        # TODO: Rename target_bbox to target_deltas for clarity\n",
    "        names = [\"rois\", \"target_class_ids\", \"target_bbox\", \"target_mask\"]\n",
    "        outputs = batch_slice(\n",
    "            [proposals, gt_class_ids, gt_boxes, gt_masks],\n",
    "            lambda w, x, y, z: detection_targets_graph(\n",
    "                w, x, y, z, self.config),\n",
    "            self.config.IMAGES_PER_GPU, names=names)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [\n",
    "            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # rois\n",
    "            (None, self.config.TRAIN_ROIS_PER_IMAGE),  # class_ids\n",
    "            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # deltas\n",
    "            (None, self.config.TRAIN_ROIS_PER_IMAGE, self.config.MASK_SHAPE[0],\n",
    "             self.config.MASK_SHAPE[1])  # masks\n",
    "        ]\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return [None, None, None, None]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRCNN(modellib.MaskRCNN):\n",
    "    def __init__(self, mode, config, model_dir):\n",
    "        assert mode in ['training', 'inference']\n",
    "        self.mode = mode\n",
    "        self.config = config\n",
    "        self.model_dir = model_dir\n",
    "        self.set_log_dir()\n",
    "    \n",
    "    \n",
    "    def build(self, mode, config):\n",
    "        assert mode in ['training', 'inference']\n",
    "\n",
    "        # Image size must be dividable by 2 multiple times\n",
    "        h, w = config.IMAGE_SHAPE[:2]\n",
    "        if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n",
    "            raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
    "                            \"to avoid fractions when downscaling and upscaling.\"\n",
    "                            \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
    "\n",
    "        # Inputs\n",
    "        input_image = KL.Input(\n",
    "            shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\")\n",
    "        input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE],\n",
    "                                    name=\"input_image_meta\")\n",
    "        if mode == \"training\":\n",
    "            # RPN GT\n",
    "            input_rpn_match = KL.Input(\n",
    "                shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32)\n",
    "            input_rpn_bbox = KL.Input(\n",
    "                shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32)\n",
    "\n",
    "            # Detection GT (class IDs, bounding boxes, and masks)\n",
    "            # 1. GT Class IDs (zero padded)\n",
    "            input_gt_class_ids = KL.Input(\n",
    "                shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32)\n",
    "            # 2. GT Boxes in pixels (zero padded)\n",
    "            # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates\n",
    "            input_gt_boxes = KL.Input(\n",
    "                shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32)\n",
    "            # Normalize coordinates\n",
    "            gt_boxes = KL.Lambda(lambda x: modellib.norm_boxes_graph(\n",
    "                x, K.shape(input_image)[1:3]))(input_gt_boxes)\n",
    "            # 3. GT Masks (zero padded)\n",
    "            # [batch, height, width, MAX_GT_INSTANCES]\n",
    "            if config.USE_MINI_MASK:\n",
    "                input_gt_masks = KL.Input(\n",
    "                    shape=[config.MINI_MASK_SHAPE[0],\n",
    "                           config.MINI_MASK_SHAPE[1], None],\n",
    "                    name=\"input_gt_masks\", dtype=bool)\n",
    "            else:\n",
    "                input_gt_masks = KL.Input(\n",
    "                    shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None],\n",
    "                    name=\"input_gt_masks\", dtype=bool)\n",
    "        elif mode == \"inference\":\n",
    "            # Anchors in normalized coordinates\n",
    "            input_anchors = KL.Input(shape=[None, 4], name=\"input_anchors\")\n",
    "\n",
    "        # Build the shared convolutional layers.\n",
    "        # Bottom-up Layers\n",
    "        # Returns a list of the last layers of each stage, 5 in total.\n",
    "        # Don't create the thead (stage 5), so we pick the 4th item in the list.\n",
    "        if callable(config.BACKBONE):\n",
    "            _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,\n",
    "                                                train_bn=config.TRAIN_BN)\n",
    "        else:\n",
    "            _, C2, C3, C4, C5 = modellib.resnet_graph(input_image, config.BACKBONE,\n",
    "                                             stage5=True, train_bn=config.TRAIN_BN)\n",
    "        # Top-down Layers\n",
    "        # TODO: add assert to varify feature map sizes match what's in config\n",
    "        P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5)\n",
    "        P4 = KL.Add(name=\"fpn_p4add\")([\n",
    "            KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5),\n",
    "            KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)])\n",
    "        P3 = KL.Add(name=\"fpn_p3add\")([\n",
    "            KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4),\n",
    "            KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)])\n",
    "        P2 = KL.Add(name=\"fpn_p2add\")([\n",
    "            KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3),\n",
    "            KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)])\n",
    "        # Attach 3x3 conv to all P layers to get the final feature maps.\n",
    "        P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)\n",
    "        P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)\n",
    "        P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)\n",
    "        P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5)\n",
    "        # P6 is used for the 5th anchor scale in RPN. Generated by\n",
    "        # subsampling from P5 with stride of 2.\n",
    "        P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)\n",
    "\n",
    "        # Note that P6 is used in RPN, but not in the classifier heads.\n",
    "        rpn_feature_maps = [P2, P3, P4, P5, P6]\n",
    "        mrcnn_feature_maps = [P2, P3, P4, P5]\n",
    "\n",
    "        # Anchors\n",
    "        if mode == \"training\":\n",
    "            anchors = self.get_anchors(config.IMAGE_SHAPE)\n",
    "            # Duplicate across the batch dimension because Keras requires it\n",
    "            # TODO: can this be optimized to avoid duplicating the anchors?\n",
    "            anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\n",
    "\n",
    "            # A hack to get around Keras's bad support for constants\n",
    "            class ConstLayer(tf.keras.layers.Layer):\n",
    "                def __init__(self, x, name=None):\n",
    "                    super(ConstLayer, self).__init__(name=name)\n",
    "                    self.x = tf.Variable(x)\n",
    "\n",
    "                def call(self, input):\n",
    "                    return self.x\n",
    "\n",
    "            anchors = ConstLayer(anchors, name=\"anchors\")(input_image)\n",
    "        else:\n",
    "            anchors = input_anchors\n",
    "\n",
    "        # RPN Model\n",
    "        rpn = modellib.build_rpn_model(config.RPN_ANCHOR_STRIDE,\n",
    "                              len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE)\n",
    "        # Loop through pyramid layers\n",
    "        layer_outputs = []  # list of lists\n",
    "        for p in rpn_feature_maps:\n",
    "            layer_outputs.append(rpn([p]))\n",
    "        # Concatenate layer outputs\n",
    "        # Convert from list of lists of level outputs to list of lists\n",
    "        # of outputs across levels.\n",
    "        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
    "        output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]\n",
    "        outputs = list(zip(*layer_outputs))\n",
    "        outputs = [KL.Concatenate(axis=1, name=n)(list(o))\n",
    "                   for o, n in zip(outputs, output_names)]\n",
    "\n",
    "        rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
    "\n",
    "        # Generate proposals\n",
    "        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
    "        # and zero padded.\n",
    "        proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\\n",
    "            else config.POST_NMS_ROIS_INFERENCE\n",
    "        rpn_rois = modellib.ProposalLayer(\n",
    "            proposal_count=proposal_count,\n",
    "            nms_threshold=config.RPN_NMS_THRESHOLD,\n",
    "            name=\"ROI\",\n",
    "            config=config)([rpn_class, rpn_bbox, anchors])\n",
    "\n",
    "        if mode == \"training\":\n",
    "            # Class ID mask to mark class IDs supported by the dataset the image\n",
    "            # came from.\n",
    "            active_class_ids = KL.Lambda(\n",
    "                lambda x: modellib.parse_image_meta_graph(x)[\"active_class_ids\"]\n",
    "                )(input_image_meta)\n",
    "\n",
    "            if not config.USE_RPN_ROIS:\n",
    "                # Ignore predicted ROIs and use ROIs provided as an input.\n",
    "                input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4],\n",
    "                                      name=\"input_roi\", dtype=np.int32)\n",
    "                # Normalize coordinates\n",
    "                target_rois = KL.Lambda(lambda x: norm_boxes_graph(\n",
    "                    x, K.shape(input_image)[1:3]))(input_rois)\n",
    "            else:\n",
    "                target_rois = rpn_rois\n",
    "             # Generate detection targets\n",
    "            # Subsamples proposals and generates target outputs for training\n",
    "            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n",
    "            # padded. Equally, returned rois and targets are zero padded.\n",
    "            # return target_rois, input_gt_class_ids, gt_boxes, input_gt_masks\n",
    "            rois, target_class_ids, target_bbox, target_mask =\\\n",
    "                DetectionTargetLayer(config, name=\"proposal_targets\")([\n",
    "                    target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model = MRCNN(mode='training',config=config, model_dir=temp_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(mode, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast(tf.constant([]), tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.argmax(input=positive_overlaps, axis=1),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
