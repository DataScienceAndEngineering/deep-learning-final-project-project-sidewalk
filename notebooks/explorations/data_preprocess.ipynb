{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/interim')\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio as iio\n",
    "import json\n",
    "from skimage.draw import polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for labels of interest\n",
    "# labels of interest\n",
    "labels_dict = {\n",
    "    8: 'sidewalk',\n",
    "    12: 'wall',\n",
    "    13: 'fence',\n",
    "    14: 'guard rail',\n",
    "    17: 'pole',\n",
    "    18:  'polegroup',\n",
    "    20: 'traffic sign'\n",
    "}\n",
    "\n",
    "labels_dc ={'sidewalk':8,\n",
    "            'rail track':10,\n",
    "            'wall':12,\n",
    "            'fence':13,\n",
    "            'guard rail':14,\n",
    "            'pole':17,\n",
    "            'polegroup':18,\n",
    "            'traffic sign': 20,\n",
    "            'vegetation': 21}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = os.path.join(DATA_DIR, 'ZIP.leftImg8bit_trainvaltest.zip/leftImg8bit')\n",
    "LBL_DIR = os.path.join(DATA_DIR, 'ZIP.gtFine_trainvaltest.zip/gtFine/')\n",
    "\n",
    "x_train, y_train = IMG_DIR + '/train', LBL_DIR+'/train'\n",
    "x_test, y_test = IMG_DIR + '/test', LBL_DIR+'/test'\n",
    "x_val , y_val =IMG_DIR + '/val', LBL_DIR+'/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "def parse_json(filename: str, keep:Union[tuple,list,dict]=['sidewalk']):\n",
    "    polygons = []\n",
    "    labels = []\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "        im_h, im_w = data['imgHeight'], data['imgWidth']\n",
    "        objects = data[\"objects\"]\n",
    "        for i in objects:\n",
    "            if i['label'] in keep:\n",
    "                # if isinstance(keep, dict):\n",
    "                #     labels.append(keep[i['label']])\n",
    "                # else:\n",
    "                #     labels.append(i['label'])\n",
    "                poly_coords = i['polygon']\n",
    "                tmp = list(zip(*poly_coords))\n",
    "            # NOTE: The following line assumes that the point coordinates are given as (x,y). \n",
    "            #       Change the order of the indices if needed.\n",
    "                polygons.append((np.array(tmp[1])-1, np.array(tmp[0])-1))\n",
    "    # return (im_h, im_w), labels, polygons\n",
    "    return (im_h, im_w), polygons\n",
    "\n",
    "# https://stackoverflow.com/questions/72168663/setting-a-list-of-x-y-cooordinates-into-an-array-so-polygons-are-drawn\n",
    "def drawMask(imgsize: tuple, poly:tuple):\n",
    "    img = np.zeros(imgsize)\n",
    "    r = poly[0]\n",
    "    c = poly[1]\n",
    "    r_index, c_index = polygon(r, c)\n",
    "    img[r_index,c_index] = 1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11ce04fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdim, polygons = parse_json(os.path.join(DATA_DIR,'gtFine/train/aachen/aachen_000000_000019_gtFine_polygons.json'))\n",
    "m = drawMask(imdim, polygons[0])\n",
    "plt.imshow(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get a list of file names from x\n",
    "# # find y json file \n",
    "# # find # of instances no need to generate masks yet\n",
    "# # img path | json path | class\n",
    "\n",
    "# def getIds(subset:list):\n",
    "#     cities = os.listdir(subset)\n",
    "#     ids = []\n",
    "#     for city in cities:\n",
    "#         for p in os.listdir(os.path.join(subset, city)): \n",
    "#             id = '_'.join(p.split('_')[:3])\n",
    "#             ids.append(id)   \n",
    "#     return ids\n",
    "\n",
    "\n",
    "# def getInstances(filename:str, labelsToKeep:dict):\n",
    "#     labels = []\n",
    "#     keys = labelsToKeep.keys()\n",
    "#     data = json.load(open(filename))\n",
    "#     data = data[\"objects\"]\n",
    "#     for d in data:\n",
    "#         l = d['label']\n",
    "#         if l in keys:\n",
    "#             labels.append(labelsToKeep[l])\n",
    "#     return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids = getIds(x_train)\n",
    "# test_ids = getIds(x_test)\n",
    "# val_ids = getIds(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getLabels(ids, x,y,keepLabels={'sidewalk':8}):\n",
    "#     labels = []\n",
    "#     for id in ids:\n",
    "#         city= id.split('_')[0]\n",
    "#         # im_path = os.path.join(x,city,id+'_leftImg8bit.png')\n",
    "#         lb_path = os.path.join(y,city, id+'_gtFine_polygons.json')\n",
    "#         labels.append(getInstances(lb_path,keepLabels))\n",
    "#     return labels\n",
    "\n",
    "# def generateCSV(filepath:str, ids, labels):\n",
    "#     df = pd.DataFrame(data=list(zip(ids, labels)),columns=['ID','LABEL'])\n",
    "#     df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_labels = getLabels(val_ids, x_val,y_val)\n",
    "# generateCSV('/Users/csea/Desktop/deep-learning-final-project-project-sidewalk/data/interim/val.csv',val_ids,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.maskrcnn.mrcnn import visualize\n",
    "from models.maskrcnn.mrcnn import model\n",
    "# from models.maskrcnn.mrcnn import log\n",
    "from models.maskrcnn.mrcnn import config\n",
    "from models.maskrcnn.mrcnn.config import Config\n",
    "from models.maskrcnn.mrcnn import model as modellib, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConfig(Config):\n",
    "  NAME = 'sidewalks'\n",
    "  GPU_COUNT = 1\n",
    "\n",
    "  # 12GB GPU can typical handle 2 images (1024 x 1024)\n",
    "  # ColabPro High Ram has roughly 27GB GPU\n",
    "  IMAGES_PER_GPU = 4\n",
    "\n",
    "  # Background + 1 sidewalk class \n",
    "  NUM_CLASSES = 2\n",
    "\n",
    "  # Number of training steps per epoch\n",
    "  STEPS_PER_EPOCH = 10\n",
    "\n",
    "  # Number of validation steps to runa tt eh end of \n",
    "  # every epoch,larger number improves better accuracy\n",
    "  # but slows down training\n",
    "  VALIDATION_STEPS = 50\n",
    "\n",
    "  # ROIs belwo this threshold are skipped\n",
    "  DETECTION_MIN_CONFIDENCE = .7   \n",
    "  DETECTION_NMS_THRESHOLD = 0.3\n",
    "  LEARNING_RATE =0.0005\n",
    "  BACKBONE = 'resnet101'\n",
    "  IMAGE_MAX_DIM = 512\n",
    "  IMAGE_MIN_DIM = 512\n",
    "  \n",
    "class CustomDataset(utils.Dataset):\n",
    " \n",
    "  def add_image(self, source, image_id, path, **kwargs):\n",
    "        image_info = {image_id:\n",
    "            {\"source\": source,\n",
    "            \"path\": path,\n",
    "            }\n",
    "        }\n",
    "        image_info[image_id].update(kwargs)\n",
    "        self.image_info.update(image_info)\n",
    "\n",
    "  def add_class(self, source, class_id, class_name):\n",
    "    assert \".\" not in source, \"Source name cannot contain a dot\"\n",
    "    # Does the class exist already?\n",
    "    if class_id in self.class_info.keys():\n",
    "       return\n",
    "\n",
    "    # Add the class\n",
    "    self.class_info.update({ class_id:\n",
    "       { \"source\": source,\n",
    "        \"name\": class_name,\n",
    "        'id': class_id}\n",
    "    })\n",
    "        \n",
    "  def load_dataset(self,dataset_dir,subset):\n",
    "    self.image_info = {}\n",
    "    self.class_info = {}\n",
    "    self.data_directory = dataset_dir\n",
    "\n",
    "    classes = pd.read_csv(os.path.join(dataset_dir,'classes.csv'))\n",
    "    IDs, LABELS = classes['CLASS_ID'].tolist(), classes['CLASS_NAME'].tolist()\n",
    "    classes = dict(zip(IDs, LABELS))\n",
    "\n",
    "    for i,c in enumerate(classes):\n",
    "      self.add_class(source='cityscape',\n",
    "                   class_id = c,\n",
    "                   class_name=classes[c])\n",
    "      \n",
    "    self.class_names = [d['name'] for d in self.class_info.values()]\n",
    "    \n",
    "    # iterating to get the image ids\n",
    "    if subset == 'validation':\n",
    "      subset = 'val'\n",
    "\n",
    "    data = pd.read_csv(os.path.join(dataset_dir, subset+'.csv'))  \n",
    "\n",
    "    for i in range(len(data)):\n",
    "      ID, LABELS = data.loc[i]\n",
    "      city = ID.split('_')[0]\n",
    "      img_path = os.path.join(dataset_dir, f'leftImg8bit/{subset}/{city}/{ID}_leftImg8bit.png')\n",
    "      classes = [int(i) if i!='' else 0 for i in LABELS.strip('[]').split(',')]\n",
    "      self.add_image(source='cityscape',\n",
    "                     image_id = ID,\n",
    "                     path = img_path,\n",
    "                     classes = classes)\n",
    "      \n",
    "  def load_mask(self,id):\n",
    "    img_info = self.image_info[id]\n",
    "    path = img_info['path']\n",
    "    subset = path.split('/')[-3]\n",
    "    mask_dir = os.path.join(self.data_directory, f'gtFine/{subset}')\n",
    "    \n",
    "    city = id.split('_')[0]\n",
    "    mask_path = os.path.join(mask_dir, city,f'{id}_gtFine_polygons.json')\n",
    "    # mask_path = os.path.join(mask_dir, city,f'{id}_gtFine_labelIds.png')\n",
    "    def parse_json(filename: str, keep=self.class_names) :\n",
    "        polygons = []\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "            im_h, im_w = data['imgHeight'], data['imgWidth']\n",
    "            objects = data[\"objects\"]\n",
    "            for i in objects:\n",
    "                if i['label'] in keep:\n",
    "                    print(i['label'])\n",
    "                    poly_coords = i['polygon']\n",
    "                    tmp = list(zip(*poly_coords))\n",
    "                    polygons.append((np.array(tmp[1])-1, np.array(tmp[0])-1))\n",
    "        # return (im_h, im_w), labels, polygons\n",
    "        return (im_h, im_w), polygons\n",
    "\n",
    "    # https://stackoverflow.com/questions/72168663/setting-a-list-of-x-y-cooordinates-into-an-array-so-polygons-are-drawn\n",
    "    def drawMask(imgsize: tuple, poly:tuple):\n",
    "        img = np.zeros(imgsize)\n",
    "        row, col = polygon(poly[0], poly[1])\n",
    "        img[row,col] = 1\n",
    "        return img\n",
    "    \n",
    "    masks = []\n",
    "    \n",
    "    imsize, polygons = parse_json(mask_path)\n",
    "    for p in polygons:\n",
    "       mask = drawMask(imsize, p)\n",
    "       masks.append(mask)\n",
    "      \n",
    "    masks = np.stack(masks, axis=-1)\n",
    "    return masks, img_info['classes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m d \u001b[39m=\u001b[39m CustomDataset()\n\u001b[1;32m      2\u001b[0m d\u001b[39m.\u001b[39mload_dataset(DATA_DIR,\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m d\u001b[39m.\u001b[39;49mprepare()\n",
      "File \u001b[0;32m~/Desktop/deep-learning-final-project-project-sidewalk/models/maskrcnn/mrcnn/utils.py:308\u001b[0m, in \u001b[0;36mDataset.prepare\u001b[0;34m(self, class_map)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_info)\n\u001b[1;32m    307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m--> 308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_names \u001b[39m=\u001b[39m [clean_name(c[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_info]\n\u001b[1;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_images \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_info)\n\u001b[1;32m    310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_images)\n",
      "File \u001b[0;32m~/Desktop/deep-learning-final-project-project-sidewalk/models/maskrcnn/mrcnn/utils.py:308\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_info)\n\u001b[1;32m    307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m--> 308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_names \u001b[39m=\u001b[39m [clean_name(c[\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_info]\n\u001b[1;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_images \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_info)\n\u001b[1;32m    310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_images)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "d = CustomDataset()\n",
    "d.load_dataset(DATA_DIR,'train')\n",
    "d.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a,b \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mload_mask(\u001b[39m'\u001b[39m\u001b[39maachen_000000_000019\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m c \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mload_image(\u001b[39m'\u001b[39m\u001b[39maachen_000000_000019\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m visualize\u001b[39m.\u001b[39mdisplay_top_masks(c,a,b,d\u001b[39m.\u001b[39mclass_info)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "a,b = d.load_mask('aachen_000000_000019')\n",
    "c = d.load_image('aachen_000000_000019')\n",
    "visualize.display_top_masks(c,a,b,d.class_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "            \"\"\"Returns a shorter version of object names for cleaner display.\"\"\"\n",
    "            return \",\".join(name.split(\",\")[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {'source': 'cityscape', 'name': 'sidewalk', 'id': 8}}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sidewalk\n"
     ]
    }
   ],
   "source": [
    "for i in d.class_info:\n",
    "    print(d.class_info[i]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sidewalk']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clean_name(d.class_info[c]['name']) for c in d.class_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.num_classes = len(d.class_info)\n",
    "d.class_ids = np.arange(d.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = d.image_info\n",
    "image_ids = np.arange(len(d.image_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m {\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(d\u001b[39m.\u001b[39mimage_info[info][\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m], d\u001b[39m.\u001b[39mimage_info[info][\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]): \u001b[39mid\u001b[39m\n\u001b[1;32m      2\u001b[0m                                       \u001b[39mfor\u001b[39;00m info, \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(d\u001b[39m.\u001b[39mimage_info, image_ids)}\n",
      "Cell \u001b[0;32mIn[258], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m {\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(d\u001b[39m.\u001b[39mimage_info[info][\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m], d\u001b[39m.\u001b[39;49mimage_info[info][\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m]): \u001b[39mid\u001b[39m\n\u001b[1;32m      2\u001b[0m                                       \u001b[39mfor\u001b[39;00m info, \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(d\u001b[39m.\u001b[39mimage_info, image_ids)}\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "{\"{}.{}\".format(d.image_info[info]['source'], info): id\n",
    "                                      for info, id in zip(d.image_info, image_ids)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = utils.extract_bboxes(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     4\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 4\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.0005\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           sidewalks\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                10\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[264], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset \u001b[39m=\u001b[39m CustomDataset()\n\u001b[1;32m      5\u001b[0m dataset\u001b[39m.\u001b[39mload_dataset(DATA_DIR,\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m dataset\u001b[39m.\u001b[39;49mprepare()\n\u001b[1;32m      8\u001b[0m val_set \u001b[39m=\u001b[39m CustomDataset()\n\u001b[1;32m      9\u001b[0m val_set\u001b[39m.\u001b[39mload_dataset(DATA_DIR,\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/deep-learning-final-project-project-sidewalk/models/maskrcnn/mrcnn/utils.py:308\u001b[0m, in \u001b[0;36mDataset.prepare\u001b[0;34m(self, class_map)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_info)\n\u001b[1;32m    307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m--> 308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_names \u001b[39m=\u001b[39m [clean_name(c[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_info]\n\u001b[1;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_images \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_info)\n\u001b[1;32m    310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_images)\n",
      "File \u001b[0;32m~/Desktop/deep-learning-final-project-project-sidewalk/models/maskrcnn/mrcnn/utils.py:308\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_info)\n\u001b[1;32m    307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m--> 308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_names \u001b[39m=\u001b[39m [clean_name(c[\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_info]\n\u001b[1;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_images \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_info)\n\u001b[1;32m    310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_images)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "config = CustomConfig()\n",
    "config.display()\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataset.load_dataset(DATA_DIR,'train')\n",
    "dataset.prepare()\n",
    "\n",
    "val_set = CustomDataset()\n",
    "val_set.load_dataset(DATA_DIR,'val')\n",
    "# val_set.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not build a TypeSpec for <KerasTensor: shape=(None, None, 4) dtype=float32 (created by layer 'tf.math.truediv_1')> with type KerasTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[225], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m temp_log \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ROOT_DIR,\u001b[39m'\u001b[39m\u001b[39mdata/external\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m modellib\u001b[39m.\u001b[39;49mMaskRCNN(mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m,config\u001b[39m=\u001b[39;49mconfig, model_dir\u001b[39m=\u001b[39;49mtemp_log)\n\u001b[1;32m      3\u001b[0m model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ROOT_DIR,\u001b[39m'\u001b[39m\u001b[39mmodels/maskrcnn/mask_rcnn_coco.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/deep-learning-final-project-project-sidewalk/models/maskrcnn/mrcnn/model.py:1837\u001b[0m, in \u001b[0;36mMaskRCNN.__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_dir \u001b[39m=\u001b[39m model_dir\n\u001b[1;32m   1836\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_log_dir()\n\u001b[0;32m-> 1837\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeras_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild(mode\u001b[39m=\u001b[39;49mmode, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/Desktop/deep-learning-final-project-project-sidewalk/models/maskrcnn/mrcnn/model.py:1875\u001b[0m, in \u001b[0;36mMaskRCNN.build\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   1872\u001b[0m input_gt_boxes \u001b[39m=\u001b[39m KL\u001b[39m.\u001b[39mInput(\n\u001b[1;32m   1873\u001b[0m     shape\u001b[39m=\u001b[39m[\u001b[39mNone\u001b[39;00m, \u001b[39m4\u001b[39m], name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_gt_boxes\u001b[39m\u001b[39m\"\u001b[39m, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m   1874\u001b[0m \u001b[39m# Normalize coordinates\u001b[39;00m\n\u001b[0;32m-> 1875\u001b[0m gt_boxes \u001b[39m=\u001b[39m KL\u001b[39m.\u001b[39;49mLambda(\u001b[39mlambda\u001b[39;49;00m x: norm_boxes_graph(\n\u001b[1;32m   1876\u001b[0m     x, K\u001b[39m.\u001b[39;49mshape(input_image)[\u001b[39m1\u001b[39;49m:\u001b[39m3\u001b[39;49m]))(input_gt_boxes)\n\u001b[1;32m   1877\u001b[0m \u001b[39m# 3. GT Masks (zero padded)\u001b[39;00m\n\u001b[1;32m   1878\u001b[0m \u001b[39m# [batch, height, width, MAX_GT_INSTANCES]\u001b[39;00m\n\u001b[1;32m   1879\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mUSE_MINI_MASK:\n",
      "File \u001b[0;32m~/miniconda/envs/tf_m1/lib/python3.8/site-packages/keras/engine/base_layer.py:976\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[39m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[39m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[39m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[39mif\u001b[39;00m _in_functional_construction_mode(\u001b[39mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[0;32m--> 976\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m    977\u001b[0m                                             input_list)\n\u001b[1;32m    979\u001b[0m \u001b[39m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[1;32m    980\u001b[0m call_context \u001b[39m=\u001b[39m base_layer_utils\u001b[39m.\u001b[39mcall_context()\n",
      "File \u001b[0;32m~/miniconda/envs/tf_m1/lib/python3.8/site-packages/keras/engine/base_layer.py:1114\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     training_arg_passed_by_framework \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[39mwith\u001b[39;00m call_context\u001b[39m.\u001b[39menter(\n\u001b[1;32m   1112\u001b[0m     layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39minputs, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39mtraining_value):\n\u001b[1;32m   1113\u001b[0m   \u001b[39m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m   outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_tensor_symbolic_call(\n\u001b[1;32m   1115\u001b[0m       inputs, input_masks, args, kwargs)\n\u001b[1;32m   1117\u001b[0m   \u001b[39mif\u001b[39;00m outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mA layer\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39ms `call` method should return a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1119\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mTensor or a list of Tensors, not None \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1120\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m(layer: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m).\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/tf_m1/lib/python3.8/site-packages/keras/engine/base_layer.py:848\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(keras_tensor\u001b[39m.\u001b[39mKerasTensor, output_signature)\n\u001b[1;32m    847\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 848\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_output_signature(inputs, args, kwargs, input_masks)\n",
      "File \u001b[0;32m~/miniconda/envs/tf_m1/lib/python3.8/site-packages/keras/engine/base_layer.py:893\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[1;32m    891\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks,\n\u001b[1;32m    892\u001b[0m                           build_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 893\u001b[0m   outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[1;32m    894\u001b[0m       keras_tensor\u001b[39m.\u001b[39;49mkeras_tensor_from_tensor, outputs)\n\u001b[1;32m    896\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_set_inputs\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs:\n\u001b[1;32m    897\u001b[0m   \u001b[39m# TODO(kaftan): figure out if we need to do this at all\u001b[39;00m\n\u001b[1;32m    898\u001b[0m   \u001b[39m# Subclassed network: explicitly set metadata normally set by\u001b[39;00m\n\u001b[1;32m    899\u001b[0m   \u001b[39m# a call to self._set_inputs().\u001b[39;00m\n\u001b[1;32m    900\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_inputs(inputs, outputs)\n",
      "File \u001b[0;32m~/miniconda/envs/tf_m1/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda/envs/tf_m1/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda/envs/tf_m1/lib/python3.8/site-packages/keras/engine/keras_tensor.py:584\u001b[0m, in \u001b[0;36mkeras_tensor_from_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    581\u001b[0m     keras_tensor_cls \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\n\u001b[1;32m    582\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m out \u001b[39m=\u001b[39m keras_tensor_cls\u001b[39m.\u001b[39;49mfrom_tensor(tensor)\n\u001b[1;32m    586\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m'\u001b[39m\u001b[39m_keras_mask\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    587\u001b[0m   out\u001b[39m.\u001b[39m_keras_mask \u001b[39m=\u001b[39m keras_tensor_from_tensor(tensor\u001b[39m.\u001b[39m_keras_mask)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/tf_m1/lib/python3.8/site-packages/keras/engine/keras_tensor.py:172\u001b[0m, in \u001b[0;36mKerasTensor.from_tensor\u001b[0;34m(cls, tensor)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m   \u001b[39m# Fallback to the generic arbitrary-typespec KerasTensor\u001b[39;00m\n\u001b[1;32m    171\u001b[0m   name \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(tensor, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 172\u001b[0m   type_spec \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mtype_spec_from_value(tensor)\n\u001b[1;32m    173\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(type_spec, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda/envs/tf_m1/lib/python3.8/site-packages/tensorflow/python/framework/type_spec.py:609\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    606\u001b[0m   logging\u001b[39m.\u001b[39mvlog(\n\u001b[1;32m    607\u001b[0m       \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[0;32m--> 609\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not build a TypeSpec for \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    610\u001b[0m                 (value, \u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for <KerasTensor: shape=(None, None, 4) dtype=float32 (created by layer 'tf.math.truediv_1')> with type KerasTensor"
     ]
    }
   ],
   "source": [
    "temp_log = os.path.join(ROOT_DIR,'data/external')\n",
    "model = modellib.MaskRCNN(mode='training',config=config, model_dir=temp_log)\n",
    "model_path = os.path.join(ROOT_DIR,'models/maskrcnn/mask_rcnn_coco.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
