# -*- coding: utf-8 -*-
"""data_exploration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/DataScienceAndEngineering/deep-learning-final-project-project-sidewalk/blob/nicholas/notebooks/data_exploration.ipynb
"""

# Loading all necessary packages
import tensorflow as tf
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import random
from time import sleep


#%% Loss Functions
def dice_coef(y_true, y_pred):
    smooth = 1e-6
    y_true_f = tf.keras.layers.Flatten()(y_true)
    y_pred_f = tf.keras.layers.Flatten()(y_pred)    

    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    denom =(tf.reduce_sum(y_pred_f) + tf.reduce_sum(y_true_f) + smooth)
    return (2. * intersection + smooth) / denom

def dice_sidewalk(y_true, y_pred):
    return dice_coef(y_true[:,:,:,0], y_pred[:,:,:,0])
    
def dice_score(y_true, y_pred, numLabels=10):
    dice=0
    for index in range(numLabels):
        dice -= dice_coef(y_true[:,:,:,index], y_pred[:,:,:,index])
    return numLabels + dice

#%% Input Generator Attempt 4 - custom generator approach
class DataGenerator(tf.keras.utils.Sequence):
  def __init__(self, image_dim=(256, 512), batch_size=4, partition='train'):
    np.random.seed(0)
    tf.random.set_seed(0)

    self.batch_size = batch_size
    self.partition = partition
    self.segpath = '/home/nicholas/Documents/Project_Sidewalk/masks/'
    self.image_path = f'/home/nicholas/Documents/Project_Sidewalk/images/{self.partition}'
    self.label_path = [f'{self.segpath}sidewalk/{self.partition}',
                       f'{self.segpath}building/{self.partition}',
                       f'{self.segpath}wall/{self.partition}',
                       f'{self.segpath}fence/{self.partition}',
                       f'{self.segpath}guard_rail/{self.partition}',
                       f'{self.segpath}bridge/{self.partition}',
                       f'{self.segpath}tunnel/{self.partition}',
                       f'{self.segpath}vegetation/{self.partition}',
                       f'{self.segpath}pole/{self.partition}',
                       f'{self.segpath}polegroup/{self.partition}']
                       
    self.list_files = os.listdir(self.image_path)
    self.image_dim = image_dim
    self.on_epoch_end()

  def __len__(self):
    return int(np.floor(len(self.list_files) / self.batch_size))

  def __getitem__(self, index):
    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
    list_files_tmp = [self.list_files[k] for k in indexes]
    X, y = self.data_generation(list_files_tmp)
    return X, y

  def on_epoch_end(self):
    self.indexes = np.arange(len(self.list_files))
    self.current_batch = 0

  def data_generation(self, list_files_tmp):
    y = np.empty((self.batch_size, *self.image_dim, 10))
    X = np.empty((self.batch_size, *self.image_dim, 3))
    for i, ID in enumerate(list_files_tmp):
      #mask = np.array(PIL.Image.open(f'/content/seperated_labels/train/{ID}')).resize(self.image_dim)
      for j in range(0,len(self.label_path)):
          y[i,:,:,j] = np.squeeze(np.load(f'{self.label_path[j]}/{ID}'))
      #y[i,] = np.expand_dims(mask, axis=-1)
      #X[i,] = np.array(PIL.Image.open(f'/content/seperated_images/train/{ID}')).astype('float32').resize(self.image_dim)
      X[i,] = np.load(f'{self.image_path}/{ID}')
    y = 1.*(y!=0)
    return X, y
  
train_generator = DataGenerator()
val_generator = DataGenerator(partition='val')

#generator = DataGenerator(list_files=all_training, image_path='/content/seperated_images/train', label_path='/content/seperated_labels/train/')
X, y = train_generator[0]

#%% plot some samples
for i in range(train_generator.batch_size):
    plt.figure()
    plt.subplot(1,2,1)
    plt.imshow(X[i])
    plt.subplot(1,2,2)
    plt.imshow(y[i,:,:,0], vmin=0, vmax=1)
    plt.show()

#%%
class MyHistoryL(tf.keras.callbacks.Callback):
    #Saving the output path and session name into the object
    def __init__(self, OUT, NAME, DEBUG=False):
        self.OUT = OUT
        self.NAME = NAME        
        self.DEBUG = DEBUG
    
    #Initialize lists when training begins
    def on_train_begin(self, logs={}):
        self.loss = []
        self.val_loss = []
    
    #Takes actions at the end of each eopch of training
    def on_epoch_end(self, epoch, logs={}):
        #Appending current metric values to lists
        self.loss.append(logs.get('loss'))
        self.val_loss.append(logs.get('val_loss'))
        
        #Generating training progress figure
        plt.figure(figsize=(10,12))
        
        #Displaying trainign loss
        plt.title('Loss')
        plt.plot(self.loss); plt.grid()
        plt.plot(self.val_loss); plt.grid()
        plt.legend(['Train','Val'])
        plt.grid()
        plt.xlabel('Epochs')
        
        #Setting layout to tight for the figure
        plt.tight_layout()
        
        #Saving the produced figure into the model folder, titled ["Training_curves.png"]
        plt.savefig(self.OUT + self.NAME + '/Training_curves.png')
        plt.close()
        
#Custom function to freeze all the layers of a model
def freeze_layers(model):
    model_type = type(model) 
    for i in model.layers:
        i.trainable = False
        if type(i) == model_type:
            freeze_layers(i)
    return model


#Custom function to save model and weights (locks prior to saving)
def save_model_and_weights(model, NAME, FOLDER):    
    model_to_save = tf.keras.models.clone_model(model)
    model_to_save.set_weights(model.get_weights())
    model_to_save = freeze_layers(model_to_save)
    model_to_save.save_weights(FOLDER + NAME + '_weights.h5')
    model_to_save.save(FOLDER + NAME + '.h5')       
        
#Creating custom callback class for tracking loss and saving checkpoint models
class my_model_checkpoint(tf.keras.callbacks.Callback):
    #Initializes the object and sets initial loss list value to 999
    def __init__(self, MODEL_PATH, MODEL_NAME):
        self.MODEL_PATH = MODEL_PATH
        self.MODEL_NAME = MODEL_NAME    
        self.val_loss = [999]    
    
    #Assesses the model at the end of each epoch
    def on_epoch_end(self, epoch, logs={}):
        min_val_loss = min(self.val_loss) #Determines past minimum loss
        current_val_loss = logs.get('val_loss') #Retrieves current loss
        self.val_loss.append(current_val_loss) #Appends current loss onto historic list
        print('Min loss so far: {}, new loss: {}'.format(min_val_loss, current_val_loss))
        #Saves model when new minium loss is present
        if current_val_loss < min_val_loss :
            print('New best model! Epoch: {}'.format(epoch))
            save_model_and_weights(self.model, self.MODEL_NAME, self.MODEL_PATH)
        #Saves the last model generated
        else:
            save_model_and_weights(self.model, self.MODEL_NAME.replace("best", "last"), self.MODEL_PATH)
    
    #Saving the first model generated at the beginning of training
    def on_train_begin(self, logs={}):
        save_model_and_weights(self.model, self.MODEL_NAME.replace("best", "first"), self.MODEL_PATH)
        
        
History = MyHistoryL('./output_thick', '')
my_custom_checkpoint = my_model_checkpoint('./output_thick', MODEL_NAME='/best_model')
EarlyStop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
                                                min_delta=0, # 0
                                                patience=15, # 15
                                                verbose=1, # 1
                                                mode='min', #min
                                                baseline=None, #None
                                                restore_best_weights=False) #False
#%% Model
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation

def conv_block(inputs, filters, kernel_size=3, activation='relu', padding='same'):
    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(inputs)
    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv)
    conv = BatchNormalization(center=False, scale=False)(conv)
    return Activation('relu')(conv)

def Unet(input_shape=(256,512,3), num_classes=10):
    inputs = Input(shape=input_shape)

    #Down path
    c1 = conv_block(inputs, 64)
    p1 = MaxPooling2D(pool_size=(2, 2))(c1)

    c2 = conv_block(p1, 128)
    p2 = MaxPooling2D(pool_size=(2, 2))(c2)

    c3 = conv_block(p2, 256)
    p3 = MaxPooling2D(pool_size=(2, 2))(c3)

    c4 = conv_block(p3, 512)
    p4 = MaxPooling2D(pool_size=(2, 2))(c4)

    # Bottom
    c5 = conv_block(p4, 1024)

    #Up path
    u6 = UpSampling2D(size=(2, 2))(c5)
    u6 = Conv2D(512, 2, activation='relu', padding='same')(u6)
    c6 = Concatenate()([u6, c4])
    c6 = conv_block(c6, 512)

    u7 = UpSampling2D(size=(2, 2))(c6)
    u7 = Conv2D(256, 2, activation='relu', padding='same')(u7)
    c7 = Concatenate()([u7, c3])
    c7 = conv_block(c7, 256)

    u8 = UpSampling2D(size=(2, 2))(c7)
    u8 = Conv2D(128, 2, activation='relu', padding='same')(u8)
    c8 = Concatenate()([u8, c2])
    c8 = conv_block(c8, 128)

    u9 = UpSampling2D(size=(2, 2))(c8)
    u9 = Conv2D(64, 2, activation='relu', padding='same')(u9)
    c9 = Concatenate()([u9, c1])
    c9 = conv_block(c9, 64)

    outputs = Conv2D(num_classes, 1, activation='sigmoid')(c9)

    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
    return model
model = Unet()
model.compile(loss=dice_score, optimizer=tf.keras.optimizers.Adam(learning_rate=.1), metrics=['acc'])
model.fit(train_generator, validation_data=val_generator,epochs=100, callbacks=[History, my_custom_checkpoint, EarlyStop])
