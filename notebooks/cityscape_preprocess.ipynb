{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataScienceAndEngineering/deep-learning-final-project-project-sidewalk/blob/main/notebooks/cityscape_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BWDx3v-P5UoY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PIL\n",
        "import imageio as iio\n",
        "import numpy as np\n",
        "import shutil\n",
        "from skimage.transform import resize\n",
        "import sklearn.model_selection\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "#Notebook takes ~2hrs to run & download processed files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Obtaining raw data from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files as gfiles\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if not os.path.isdir('/content/rawdata/'):\n",
        "  os.mkdir('/content/rawdata/')\n",
        "\n",
        "def unzip(src, dst, preview_console=False):\n",
        "  #unzips the 'src' file to the 'dst' directory\n",
        "  #'preview_console' will mirror the command to the console if True\n",
        "  name = src.split('/')[-1]\n",
        "  print(f'decompressing {name}')\n",
        "  cmd = f'unzip -n -q {src} -d {dst}'\n",
        "  if preview_console:\n",
        "    print(cmd)\n",
        "  subprocess.run(cmd, shell=True)\n",
        "\n",
        "def check_dir_path(path, gen=False):\n",
        "  if path[-1] != '/':\n",
        "    path += '/'\n",
        "  if gen:\n",
        "    for i in range(2,len(path.split('/'))):\n",
        "      dir = '/'.join(path.split('/')[0:i])\n",
        "      if not os.path.isdir(dir):\n",
        "        print(f'generating: {dir}')\n",
        "        os.mkdir(dir)\n",
        "  return path\n",
        "\n",
        "#Uncompressing the raw data\n",
        "if not os.path.isdir('/content/rawdata/leftImg8bit/'):\n",
        "  unzip('/content/drive/MyDrive/tensorflow_datasets/downloads/manual/leftImg8bit_trainvaltest.zip', '/content/rawdata/')\n",
        "if not os.path.isdir('/content/rawdata/gtFine/'):\n",
        "  unzip('/content/drive/MyDrive/tensorflow_datasets/downloads/manual/gtFine_trainvaltest.zip', '/content/rawdata/')\n",
        "if not os.path.isdir('/content/rawdata/disparity/'):\n",
        "  unzip('/content/drive/MyDrive/tensorflow_datasets/downloads/manual/disparity_trainvaltest.zip', '/content/rawdata/')"
      ],
      "metadata": {
        "id": "wiBN2WF2LjlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef2a378-5663-4e06-bddc-a98fb3b4739d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vlf70kis6HKu"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import distance_matrix\n",
        "### Data Extraction (finding samples with sidewalks)\n",
        "\n",
        "def find_sidewalks(labeldir):\n",
        "  # Looks through the provided directory for labels including sidewalk segmentations\n",
        "  # Returns list of filenames which contain valid sidewalk segmentations \n",
        "  labeldir = check_dir_path(labeldir)\n",
        "  files = []\n",
        "  subsets = ['train', 'val', 'test']\n",
        "  for group in subsets:\n",
        "    Dir = f'{labeldir}{group}'\n",
        "    Dir = check_dir_path(Dir)\n",
        "    for k in os.listdir(Dir):\n",
        "      Dir2 = f'{Dir}{k}'\n",
        "      Dir2 = check_dir_path(Dir2)\n",
        "      files_list = os.listdir(Dir2)\n",
        "      files_list = [f for f in files_list if f.endswith('labelIds.png')]\n",
        "      for f in files_list:\n",
        "        if 8 in np.array(PIL.Image.open(Dir2+f)):\n",
        "          files.append(f'{group}/{k}/{f}')\n",
        "  print(f'Identified {len(files)} samples containing sidewalks')\n",
        "  return files\n",
        "  #return [files.split('/')[-1], '/'.join(files.split('/')[-2:-4])]\n",
        "\n",
        "def grab_files(src, dst, files, purge=False):\n",
        "  # Moves all files listed in 'files' from 'src' directory to 'dst' directory\n",
        "  # Builds 'dst' directory if necessary\n",
        "  src = check_dir_path(src)\n",
        "  dst = check_dir_path(dst, gen=True)\n",
        "  print(f'Moving {len(files)} files from {src} to {dst}')\n",
        "  for f in files:\n",
        "    name = f.split('/')[-1]\n",
        "    os.rename(f'{src}{f}', f'{dst}{name}')\n",
        "    time.sleep(0.01)\n",
        "  if purge:\n",
        "    shutil.rmtree(src)\n",
        "\n",
        "def download_dir(src, dst, name, preview_console=False):\n",
        "  # Download the 'src' directory as a zip file with 'name' to the 'dst' directory\n",
        "  src = check_dir_path(src)\n",
        "  dst = check_dir_path(dst)\n",
        "  cmd = f'zip -r {dst}{name}.zip {src}'\n",
        "  if preview_console:\n",
        "    print(cmd)\n",
        "  subprocess.run(cmd, shell=True)\n",
        "  gfiles.download(f'{dst}{name}.zip')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Extracting Data\n",
        "def main_extract(label_dir='/content/rawdata/gtFine/', image_dir='/content/rawdata/leftImg8bit/', depth_dir='/content/rawdata/disparity/', purge=False):\n",
        "  files_label = find_sidewalks(label_dir)\n",
        "  print(f'Extracting {len(files_label)} samples...')\n",
        "  files_image = [i.replace('gtFine_labelIds', 'leftImg8bit') for i in files_label]\n",
        "  files_depth = [i.replace('gtFine_labelIds', 'disparity') for i in files_label]\n",
        "  grab_files(label_dir, '/content/extracted/labels', files_label, purge=purge)\n",
        "  grab_files(image_dir, '/content/extracted/images', files_image, purge=purge)\n",
        "  grab_files(depth_dir, '/content/extracted/disparity', files_depth, purge=purge)"
      ],
      "metadata": {
        "id": "DVLtR2ob9swP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DuKspi0vBP0Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8TtNMg626rot"
      },
      "outputs": [],
      "source": [
        "from pandas._libs.lib import fast_unique_multiple_list_gen\n",
        "## Data Processing and Exporting\n",
        "def process_images(src, dst, shape=(256, 512, 3), purge=False):\n",
        "  #Preprocessing of the images, resizing to standard size\n",
        "  print('Starting image processing...')\n",
        "  src = check_dir_path(src)\n",
        "  dst = check_dir_path(dst, gen=True)\n",
        "  files = os.listdir(src)\n",
        "  print(f'found {len(files)} images')\n",
        "  i=0\n",
        "  for f in files:\n",
        "    image = iio.imread(src + f)\n",
        "    image = resize(image, shape)\n",
        "    np.save(f'{dst}{f[:-16]}.png', image)\n",
        "    #time.sleep(0.1)\n",
        "    i+=1\n",
        "    if i%500 == 0: print(f'{i}/{len(files)}')\n",
        "  print(f'Image processing completed, processed {i} files')\n",
        "\n",
        "def process_masks(src, dst, shape=(256, 512, 1), id=8, name='sidewalk', purge=False):\n",
        "  # Preprocessing of the masks: removes all segmentations except sidewalks\n",
        "  # REsizes masks to standard size\n",
        "  src = check_dir_path(src)\n",
        "  dst = check_dir_path(dst)+name\n",
        "  dst = check_dir_path(dst, gen=True)\n",
        "  files = os.listdir(src)\n",
        "  print(f'found {len(files)} masks')\n",
        "  i = 0\n",
        "  for f in files:\n",
        "    mask = iio.imread(src+f)\n",
        "    mask = resize(mask, shape)\n",
        "    mask = np.array(mask == id).astype('uint8') #Binarize as last step\n",
        "    f = f[:-20]+'.png'\n",
        "    np.save(dst+f, mask)\n",
        "    time.sleep(0.01)\n",
        "    i += 1\n",
        "  print(f'{name} mask processing completed, processed {i} files')\n",
        "\n",
        "def process_depth(src, dst, shape=(256, 512, 1), purge=False):\n",
        "  print('Starting depth processing...')\n",
        "  src = check_dir_path(src)\n",
        "  dst = check_dir_path(dst, gen=True)\n",
        "  files = os.listdir(src)\n",
        "  print(f'found {len(files)} depth masks')\n",
        "  i = 0\n",
        "  for f in files:\n",
        "    depth = iio.imread(src+f)\n",
        "    depth = resize(depth, shape)\n",
        "    depth = depth/np.max(depth)\n",
        "    f = f[:-14]+'.png'\n",
        "    np.save(dst+f, depth)\n",
        "    time.sleep(0.01)\n",
        "    i+=1\n",
        "  print(f'Depth processing completed, processed {i} files')\n",
        "\n",
        "def main_process(image_dir='/content/extracted/images/', mask_dir='/content/extracted/labels/', depth_dir='/content/extracted/disparity/', dst='/content/processed/', purge=False):\n",
        "  #Processes all extracted files, zips images and labels, then downloads processed data\n",
        "  print('Starting processing...')\n",
        "  dst = check_dir_path(dst)\n",
        "  out_dir = dst+'images/'\n",
        "  out_dir = check_dir_path(out_dir, gen=True)\n",
        "  process_images(image_dir, out_dir, purge=purge)\n",
        "\n",
        "  #List of segmentation IDs to be extracted into masks, can be updated\n",
        "  ID_log = {\n",
        "      8:'sidewalk',\n",
        "      11:'building',\n",
        "      12:'wall',\n",
        "      13:'fence',\n",
        "      14:'guard_rail',\n",
        "      15:'bridge',\n",
        "      16:'tunnel',\n",
        "      21:'vegetation',\n",
        "      17:'pole',\n",
        "      18:'polegroup'}\n",
        "  out_dir = dst+'masks/'\n",
        "  out_dir = check_dir_path(out_dir, gen=True)\n",
        "  print('Starting mask processing...')\n",
        "  for i in ID_log.keys():\n",
        "    process_masks(src=mask_dir, dst=out_dir, id=i, name=ID_log[i])\n",
        "  if purge:\n",
        "    shutil.rmtree(mask_dir)\n",
        "\n",
        "  out_dir = dst+'depth/'\n",
        "  out_dir = check_dir_path(out_dir, gen=True)\n",
        "  process_depth(src=depth_dir, dst=out_dir, purge=purge)\n",
        "  \n",
        "  filesim = os.listdir(dst+'images/')\n",
        "  filesms = os.listdir(dst+'masks/pole/')\n",
        "  filesde = os.listdir(dst+'depth/')\n",
        "  print(f'{len(filesim)} images post-processing')\n",
        "  print(f'{len(filesms)} masks post-processing (pole directory)')\n",
        "  print(f'{len(filesde)} depth images post-processing')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Fix to allow for splitting into data partitions\n",
        "def move_subdir(src, files, subdir):\n",
        "  src = check_dir_path(src)\n",
        "  dst = check_dir_path(src+subdir, gen=True)\n",
        "  for f in files[1:]:\n",
        "    try:\n",
        "      os.rename(f'{src}{f}', f'{dst}{f}')\n",
        "    except:\n",
        "      continue\n",
        "      #print(f'missing file {f}')\n",
        "\n",
        "def split_dir(src, dict_path):\n",
        "  src = check_dir_path(src)\n",
        "  dict_path = check_dir_path(dict_path)\n",
        "  groups = ['val', 'test', 'train']\n",
        "  for i in groups:\n",
        "    files = list(pd.read_csv(f'{dict_path}{i}.csv', header=None)[1])\n",
        "    move_subdir(src, files, i)\n",
        "\n",
        "def apply_split(src='/content/processed/', dict_path='/content/drive/MyDrive/tensorflow_datasets/', download=False):\n",
        "  src = check_dir_path(src)\n",
        "  subdirs = os.listdir(src)\n",
        "  for i in subdirs:\n",
        "    src2 = src+i\n",
        "    src2 = check_dir_path(src2)\n",
        "    if i == 'masks':\n",
        "      subsubdirs = os.listdir(src2)\n",
        "      for k in subsubdirs:\n",
        "        src3 = check_dir_path(src2+k)\n",
        "        split_dir(src3, dict_path)\n",
        "    else:\n",
        "      split_dir(src2, dict_path)\n",
        "  if download:\n",
        "    for i in subdirs:\n",
        "      src_ = check_dir_path(src + i)\n",
        "      download_dir(src, '/content/drive/MyDrive/', i)\n",
        "#def split_files():\n",
        "#  files = os.listdir('/content/processed/images/')\n",
        "#  print(f'splitting total of {len(files)}')\n",
        "#  train, test_tmp = sklearn.model_selection.train_test_split(files, test_size=.2, train_size=.8, shuffle=True)\n",
        "#  test, val = sklearn.model_selection.train_test_split(test_tmp, test_size=.5, train_size=.5) \n",
        "apply_split(download=True)"
      ],
      "metadata": {
        "id": "BiT5hThJ1anZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main_extract()\n",
        "  main_process()\n",
        "  apply_split(download=True)"
      ],
      "metadata": {
        "id": "HOJJSz3L9sWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP46XskSMp/e0hlHQs21+SI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}