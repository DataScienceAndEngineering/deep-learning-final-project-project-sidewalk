{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataScienceAndEngineering/deep-learning-final-project-project-sidewalk/blob/main/docs/Project_Sidewalk_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YLdvfUQvFmFM"
      },
      "source": [
        "# Project Sidewalk Report\n",
        "##### Analyzing sidewalk obstructions through images of the street\n",
        "##### Rabiul Hossain, Nicholas Leotta, Nancy Sea\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP2h13LGJXUn"
      },
      "source": [
        "# **Abstract**\n",
        "***1-2 paragraphs of 200–250 words. Should concisely state the problem, why it is important,\n",
        "and give some indication of what you accomplished (2-3 discoveries)***\n",
        "\n",
        "This work aims to develop a system capable of quickly determining the level of accessibility of a given sidewalk. This system would assist visually impaired individuals with navigating densely obstructed walkways common in major cities and allow for autonomous navigation of dynamically shifting sidewalk environments. We utilize the Cityscapes Dataset, a collection of 5k images taken from the dashcam of a car around 50 European cities. Each image is associated with a fine, instanced annotation of 30 classes. We subsampled this dataset to approximately 3k images containing a portion of the visible, segmented sidewalk. We utilized multiple models capable of segmenting these images into component objects, planning further to process the location and identity of the objects to determine the degree of obstruction present in the image.\n",
        "\n",
        "We explored multiple methodologies to obtain segmentations; MaskRCNN, UNet, SAM (Segment Anything Model), and YOLO, with Random Forest operating as a baseline. The MaskRCNN model was the best choice, as it can produce instanced segmentations while determining the class of the segmented object. The MaskRCNN model is built on FPN and ResNet101 and is pre-trained on the MS COCO dataset, producing instanced segmentations for everyday objects. By adding some additional dense layers for classification, we attempted to train the model to utilize the segmentations to predict observed obstruction of the sidewalk."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RH2QlhtSqKIf"
      },
      "source": [
        "# **Introduction**\n",
        "***State your data and research question(s). Indicate why it is important. Describe your\n",
        "research plan so that readers can easily follow your thought process and the flow of the\n",
        "report. Please also include key results at the beginning so that readers know to look for.\n",
        "Here you can very briefly mention any important data cleaning or preparation. Do not talk\n",
        "about virtual results i.e. things you tried or wanted to do but didn’t do. Virtual results are\n",
        "worse than worthless. They highlight failure.***\n",
        "\n",
        "The inspiration for this work is Project Sidewalk from The University of Washington. The University of Washington project aims to collect a dataset containing all locations and timestamps for sidewalk obstructions and accessibility items. They plan to utilize this data for machine learning, but their public APIs only provide access to the location information of identified obstacles and objects. We plan to expand upon this idea and create a system capable of automatically determining obstructions present on a sidewalk from simple images or video frames.\n",
        "\n",
        "To simplify our research, we identified three tasks we must address. The first step was obtaining a segmented sidewalk region within an image. The second was to obtain segmentations of objects which would compose common sidewalk obstructions. The final step was to, utilizing the location and object type, determine the likelihood of each object constituting a suitable obstruction. We utilized Random Forest to obtain sidewalk segmentations from the input image to compare our model to a baseline prediction. We did not compose baseline predictions for total obstruction classification, as this problem is rather complex.\n",
        "\n",
        "The first segmentation task was relatively easy, as obtaining semantic sidewalk through the use of pre-trained, as well as custom models, was largely successful. We obtained acceptable performance on a simple sidewalk segmentation task utilizing a custom-trained U-Net.  When compared to our baseline prediction, this implementation was incredibly successful. Random forest obtained a Dice score of 0.077, while the U-Net obtained a Dice score of .72 on the same validation dataset.\n",
        "\n",
        "The second segmentation task was slightly more challenging than the initial. We required segmentations for each object in the image that could constitute a sidewalk obstruction. For each object to be processed individually, more than semantic segmentations is required. Instead, we would need to produce instanced segmentations. To accomplish this task, we explored utilizing multiple pre-trained models; MaskRCNN, Yolo, and SAM. Each model successfully obtained the required instanced segmentation from a minimally processed image; however, there were significant compatibility issues with utilizing each one. After significant tinkering, MaskRCNN was the best model to move forward within our specific development environment.\n",
        "\n",
        "Finally, the obstruction classification was the most challenging aspect of this work. Utilizing the instanced segmentation model, we needed to append additional layers to determine the level of obstruction for each segmented object. To accomplish this, we added dense layers parallel to the class and bounding box determination layers, comparing obstruction prediction to an algorithmically determined ground truth obtained from an augmented dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g5DRrRZFq2JR"
      },
      "source": [
        "# **Background**\n",
        "***Discuss other relevant work on solving this problem. Most of your references are here. Cite\n",
        " all sources. There is no specific formatting requirement for citations but be consistent.***\n",
        "\n",
        "This project is inspired mainly by Project Sidewalk from The University of Washington. This original project is focused on crowd-sourcing accessibility information for a given city through gamification of the data collection process. Their work was adequate, with human participation obtaining a recall of 63% and a precision of 71% on correctly annotating sidewalk images [Saha et al., 2019]. However, this system was optimized to collect locations of obstructions within an image but falls short of obtaining segmentations of objects for further image processing. A completely automated machine learning system could surpass these results and accurately determine sidewalk obstructions via deep learning models.\n",
        "\n",
        "Recent advances in automatically producing instanced segmentations have made this goal a possibility. New models, such as MaskRCNN [He et al., 2017] and SAM (Segment Anything Model) [Kirillov et al., 2023], can extract objects from images with incredible accuracy. We want to expand upon these models to allow the categorization of segmented objects based on relative location and identity. Utilizing these models as a starting point saves us from training an instanced segmenter from scratch, which is very intensive and demanding. For example, SAM was trained on a dataset of 11 million for either 90k or 180k iterations, depending on desired mask quality [Kirillov et al., 2023], and MaskRCNN on a dataset of 135k images for 90k iterations [He et al., 2017]. Leveraging these models provides a solid foundation for instanced segmentation, allowing us to focus on determining how to add classification to the obtained outputs. For example, we could focus on fine-tuning training for obstruction classification instead of training from scratch.\n",
        "\n",
        "Successful implementation of our proposed model would allow for automated determination of sidewalk obstructions. These obstructions would have a known classifier, size, and location, allowing for information to be shared with the necessary public service to assist in clearing or otherwise managing unexpected obstructions. The data can also be provided to individuals needing accessible routes, enabling them to plan, avoiding reaching impassable obstacles or getting mobility devices stuck and requiring external assistance.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FObMd3T7rGFM"
      },
      "source": [
        "# **Data**\n",
        "***Where you go the data. Describe the variables. You can begin discussing the data wrangling,\n",
        "and data cleaning. Some EDA may happen here. This includes your data source (including\n",
        "URL if applicable), any articles behind the data source.***\n",
        "\n",
        "We utilized the CityScapes Dataset for the purposes of this model[Cordts et al., 2016].  This dataset is composed of over 5k images collected by dashcam while driving around 50 German cities.  This dataset is extensive, however we only needed a subset of data for our purposes, notably images containing sidewalks.  After isolating and processing the data based on the sidewalk label being present in the semantic mask, we were left with slightly over 3k samples. Each sample included an image, a semantic segmentation mask for each object class, and a json file detailing polygon coordinates and class for each occurence of an object within the image (instanced segmentaiton).  After extraction, we applied a custom train/test/val split, allocating 10% of the data for both validation and testing, with the remaining 80% utilized for training.\n",
        "\n",
        "To allow for training of the obstruction classifier, we constructed an augmented dataset, algorithmatically determining obstructions utilizing intersection over union (IOU) to score the prevelance of the object within the sidewalk boundary.  Special attention was paid to cars intersecting with the sidewalk, and any overlap with the street removed it as a possible obstruction, as it is likely the car is simply parked or driving adjacent to the vehicle collecting the images.  This dataset included a total of 115,567 samples, each denoting a single object within an image.  ~84k objects were deemed non-obstructions, while the remaining ~32k was established as significant sidewalk obstructions.  Images were randomly sampled and obstructions visually verified to ensure the dataset was constructed as expected.\n",
        "\n",
        "The obstruction dataset contained a slight class imbalance, with 72% of objects deemed non-obstructions.  We attempted to augment our dataset with synthetic data to alleviate this disparity.  However, our constructed augmenter was only placing a single object on the sidewalk as an obstruction.  While this assisted with producing images containing obstructions, it did not help to alleviate the imbalance of object instances. To solve this, we would need to expand the augmenter to place additional objects, which is not feasible given the relatively small size of the sidewalk regions in the majority of images used.\n",
        "\n",
        "The final obstruction dataset contained only 10 out of the 30 classes of the original dataset. From random sampling and manual inspection, we conclude that the classes in the figure below were the most common obstructions a pedestrian could encounter. \n",
        "\n",
        "![](../reports/figures/segmentation_histogram.png)\n",
        "\n",
        "![](../reports/figures/obstruction_segmentation_hist.png)\n",
        "\n",
        "Further processing was performed on these datasets to assist with model exploration and data consistency.  First, each classes semantic mask was extracted and saved seperately, assisting with training traditional semantic masking techniques (i.e. Unet).  The images, respective mask images, and json polygon coordinates were downsampled from 2048x1024 to 512x216 to minimize data complexity and training overhead.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0OuuJDrMK8"
      },
      "source": [
        "# **Methods**\n",
        "***How did you take your data and set up the problem? Describe things like normalization,\n",
        "feature selection, the models you chose. In this section, you may have EDA and graphs\n",
        "showing the exploration of hyper-parameters. Note: Use graphs to illustrate interesting\n",
        "relationships that are important to your final analyses. DO NOT just show a bunch of\n",
        "graphs because you can. You should label and discuss every graph you include. There is no\n",
        "required number to include. The graphs should help us understand your analysis process\n",
        "and illuminate key features of the data.***\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nick's stuff here "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first model which we will call ‘Simple_Dense’ that  we choose to evaluate follows the structure of the MaskRCNN classifier head. This model has one Dense layer with two nodes - for classifying 0: non-obstructions and 1: obstructions. We call this portion of the network, the obstruction head. As the classifier head calculates the loss using the categorical cross entropy, the obstruction head utilizes binary cross entropy to propagate its losses. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MFT5jBfzrQ66"
      },
      "source": [
        "# **Evaluation**\n",
        "***Here you are going to show your different models’ performance. It is particularly useful to\n",
        "show multiple metrics and things like ROC curves (for binary classifiers). Make sure it is\n",
        "clearly not just what the score is but for which instances in the data one has the largest\n",
        "errors (in a regression), or just sample examples miss-classified. Make an attempt to\n",
        "interpret the parameters of the model to understand what was useful about the input data.\n",
        "Method comparison and sensitivity analyses are absolutely CRUCIAL to good scientific\n",
        "work. To that end, you MUST compare at least 2 different methods from class in answering\n",
        "your scientific questions. It is important to report what you tried but do so SUCCINCTLY.***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our modified MaskRCNN generates multiple outputs for each instance, the predicted bounding boxes, masks, class ids, class confidence, obstruction label, obstruction confidence. This creates a challenge to evaluate the performance of the model, leaving us to combine object detection metrics and semantic segmentation metric in hopes of giving us a comprehensive insight to how the model is performing. As explained in the above ‘Method’ section, we had made various changes to the network for the purpose of detecting and segmenting multi-label and multi-class instances. In this section, we will present the performance of models that has achieved training of 20 epochs.\n",
        "\n",
        "The initial challenge is matching the predictions to their ground truth labels. We do this by calculating the IoUs of all the instances of masks and instances of ground truth with one another. If the IoUs are above a certain threshold, we accept the pair as a candidate groundtruth, prediction pair. However, ultimately the ground truth label will be matched to the prediction with the highest IoU. \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DICE-Coefficient\n",
        "Instance and semantic segmentations has become a popular research field which builds upon the object classification and detection tasks. The DICE coefficient is a statistical measure used to calculate the similarity and overlap of two binary objects. However, the metric does not take into account the class labels but rather just the binary masks itself.\n",
        "$$\\text{Dice coefficient} = \\frac{{2 \\times \\text{intersection}}}{{\\text{sum of set A} + \\text{sum of set B}}}$$\n",
        "\n",
        "This is perfectly suited to check how well our model is doing on the mask generation task. As described above, the ground truth and prediction pair that have the highest IoU are chosen and their DICE coefficient calculated. The performance of the entire test dataset is summarized below. Each column has a different IoU threshold to filter labels and predictions pairs:\n",
        "\n",
        "|          |IOU@0.1                 | IOU@0.25                  | IOU@0.5                 | IOU@0.7                  |\n",
        "|----------|--------------------------|--------------------------|--------------------------|--------------------------|\n",
        "| Simple_Dense | \t 0.08048      |  0.07890      | 0.02687     | 0.00878     |\n",
        "| Model 2 |        0.03794           | 0.031854      |    0.009727           |                 0.0        |\n",
        "\n",
        "As predicted for both models, the higher the IoU threshold, the smaller the score becomes as the threshold filters out the majority of low IoU labels. Simple_Dense however, performed slightly better.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precision/Recall\n",
        "Precision and Recall is one of the most commonly used metrics for classification tasks. Since our project hopes to detect both binary and multi-class objects, we decided that we will also be using these metrics to evaluate and compare the performance between our models. Similarly to the DICE metric, we evaluate the detection with the matched ground truth bounding box to the predicted bounding box of the same threshold range $(0.1,0.25,0.5,0.7)$. \n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"../reports/figures/epoch10_precision_recall_iou0_1.png\" width=\"45%\">\n",
        "&nbsp; &nbsp; &nbsp; &nbsp;\n",
        "  <img src=\"../reports/figures/epoch10_precision_recall_iou0_25.png\" width=\"45%\">\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"../reports/figures/epoch10_precision_recall_iou0_5.png\" width=\"45%\">\n",
        "&nbsp; &nbsp; &nbsp; &nbsp;\n",
        "  <img src=\"../reports/figures/epoch10_precision_recall_iou0_7.png\" width=\"45%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Perhaps due to the nature of the imbalanced multi-class data, the network was only able to detect the six of the ten labels even at the lowest IoU threshold, this can be shown in the classification report below. \n",
        "\n",
        "![](../reports/figures/epoch_10_multi_class_iou10.png)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vlcb4PerX3s"
      },
      "source": [
        "# **Conclusion**\n",
        "***How well did it work? Characterize how robust you think the results are (did you have\n",
        "enough data?) Try for interpretation of what the model found (what variables were useful,\n",
        "what was not)? Try to avoid describing what you would do if you had more time. If you\n",
        "have to make a statement about “future work” limit it to one short statement.***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bks-yjy_tnlS"
      },
      "source": [
        "# **Attribution**\n",
        "***Using the number and size of github commits by author (bar graph), and the git hub\n",
        "visualizations of when the commits occurred. Using these measures each person should\n",
        "self-report how many code-hours of their work are visible in the repo with 2-3 sentences\n",
        "listing their contribution. Do not report any code hours that cannot be traced to commits. If\n",
        "you spend hours on a 2-line change of code or side-reading you did, you cannot report. If\n",
        "you do searches or research for the project that does not result in code, you must create\n",
        "notes in a markdown file (eg. in the project wiki) and the notes should be commensurate\n",
        "with the amount of work reported. Notes cannot be simply copy-pasted from elsewhere\n",
        "(obviously).***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37t3uj11S__V"
      },
      "source": [
        "#**References**\n",
        "Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., &amp; Schiele, B. (2016). The cityscapes dataset for Semantic Urban Scene understanding. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https://doi.org/10.1109/cvpr.2016.350 \n",
        "\n",
        "He, K., Gkioxari, G., Dollar, P., &amp; Girshick, R. (2017). Mask R-CNN. 2017 IEEE International Conference on Computer Vision (ICCV). https://doi.org/10.1109/iccv.2017.322 \n",
        "\n",
        "Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollár, P., & Girshick, R. (2023). Segment Anything. ArXiv:2304.02643.\n",
        "\n",
        "Saha, M., Saugstad, M., Maddali, H. T., Zeng, A., Holland, R., Bower, S., Dash, A., Chen, S., Li, A., Hara, K., &amp; Froehlich, J. (2019). Project sidewalk. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3290605.3300292 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOxeB91mzIKNE/HizOhAhw+",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
