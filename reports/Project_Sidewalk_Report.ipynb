{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzTVtVgr7QaIEh+Lb7uUsT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataScienceAndEngineering/deep-learning-final-project-project-sidewalk/blob/main/reports/Project_Sidewalk_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Sidewalk Report\n",
        "##### Analyzing sidewalk accessibility through images of the street\n",
        "##### Rabiul Hossain, Nicholas Leotta, Nancy Sea\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YLdvfUQvFmFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Abstract**\n",
        "***1-2 paragraphs of 200–250 words. Should concisely state the problem, why it is important,\n",
        "and give some indication of what you accomplished (2-3 discoveries)***\n",
        "\n",
        "This work aims to develop a system capable of quickly determining the level of accessibility of a given sidewalk. This system would assist visually impaired individuals with navigating densely obstructed walkways common in major cities and allow for autonomous navigation of dynamically shifting sidewalk environments. We utilize the Cityscapes Dataset, a collection of 5k images taken from the dashcam of a car around 50 European cities. Each image is associated with a fine, instanced annotation of 30 classes. We subsampled this dataset to approximately 3k images containing a portion of the visible, segmented sidewalk. We utilized multiple models capable of segmenting these images into component objects, planning further to process the location and identity of the objects to determine the degree of obstruction present in the image.\n",
        "\n",
        "We explored multiple methodologies to obtain segmentations; MaskRCNN, UNet, SAM (Segment Anything Model), and YOLO, with Random Forest operating as a baseline. The MaskRCNN model was the best choice, as it can produce instanced segmentations while determining the class of the segmented object. The MaskRCNN model is built on FPN and ResNet101 and is pre-trained on the MS COCO dataset, producing instanced segmentations for everyday objects. By adding some additional dense layers for classification, we attempted to train the model to utilize the segmentations to predict observed obstruction of the sidewalk."
      ],
      "metadata": {
        "id": "KP2h13LGJXUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Introduction**\n",
        "***State your data and research question(s). Indicate why it is important. Describe your\n",
        "research plan so that readers can easily follow your thought process and the flow of the\n",
        "report. Please also include key results at the beginning so that readers know to look for.\n",
        "Here you can very briefly mention any important data cleaning or preparation. Do not talk\n",
        "about virtual results i.e. things you tried or wanted to do but didn’t do. Virtual results are\n",
        "worse than worthless. They highlight failure.***\n",
        "\n",
        "The inspiration for this work is Project Sidewalk from The University of Washington.  The University of Washington project aims to collect a dataset containing all locations and timestamps for sidewalk obstructions and accessibility items. They plan to utilize this data for machine learning purposes, but their public API's only provide access to location information of identified obstacles and objects. We hope to expand upon this idea, and create a system capable of automatically determining obstructions present on a sidewalk from simple images or video frames.\n",
        "\n",
        "To simplify our research, we identified three tasks which must be completed.  The first step was to obtain a segmented region for the sidewalk within an image.  The second was to obtain segmentations of objects which would compose common sidewalk obstructions.  The final step was to, utilizing the location and object type, determine the likelihood of each object being considered a suitable obstruction. To compare our model to a baseline prediction, we utilized Random Forest to obtain a sidewalk segmentation from the input image.  We did not compose baseline predictions for total obstruction classificaiton, as this problem is rather complex.\n",
        "\n",
        "Looking through multiple datasets, we identified the CityScapes Dataset as an excellent candidate for facillitating the training of the segmentation models.  This dataset contains over 5k images of european city streets, with approximately 3k of these images containing a view of a sidewalk [Cordts, 2016]. To ease computational requirements for training, we reduced all images and masks from a base size of 1024x2048 to 256x512.  We also processed the masks, extracting individual semantic segmentations for each underlying class. This pre-processed data was utilized for the initial training of our segmentation models.\n"
      ],
      "metadata": {
        "id": "RH2QlhtSqKIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Background**\n",
        "***Discuss other relevant work on solving this problem. Most of your references are here. Cite\n",
        " all sources. There is no specific formatting requirement for citations but be consistent.***"
      ],
      "metadata": {
        "id": "g5DRrRZFq2JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data**\n",
        "***Where you go the data. Describe the variables. You can begin discussing the data wrangling,\n",
        "and data cleaning. Some EDA may happen here. This includes your data source (including\n",
        "URL if applicable), any articles behind the data source.***"
      ],
      "metadata": {
        "id": "FObMd3T7rGFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Methods**\n",
        "***How did you take your data and set up the problem? Describe things like normalization,\n",
        "feature selection, the models you chose. In this section, you may have EDA and graphs\n",
        "showing the exploration of hyper-parameters. Note: Use graphs to illustrate interesting\n",
        "relationships that are important to your final analyses. DO NOT just show a bunch of\n",
        "graphs because you can. You should label and discuss every graph you include. There is no\n",
        "required number to include. The graphs should help us understand your analysis process\n",
        "and illuminate key features of the data.***"
      ],
      "metadata": {
        "id": "1z0OuuJDrMK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Evaluation**\n",
        "***Here you are going to show your different models’ performance. It is particularly useful to\n",
        "show multiple metrics and things like ROC curves (for binary classifiers). Make sure it is\n",
        "clearly not just what the score is but for which instances in the data one has the largest\n",
        "errors (in a regression), or just sample examples miss-classified. Make an attempt to\n",
        "interpret the parameters of the model to understand what was useful about the input data.\n",
        "Method comparison and sensitivity analyses are absolutely CRUCIAL to good scientific\n",
        "work. To that end, you MUST compare at least 2 different methods from class in answering\n",
        "your scientific questions. It is important to report what you tried but do so SUCCINCTLY.***"
      ],
      "metadata": {
        "id": "MFT5jBfzrQ66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusion**\n",
        "***How well did it work? Characterize how robust you think the results are (did you have\n",
        "enough data?) Try for interpretation of what the model found (what variables were useful,\n",
        "what was not)? Try to avoid describing what you would do if you had more time. If you\n",
        "have to make a statement about “future work” limit it to one short statement.***"
      ],
      "metadata": {
        "id": "1Vlcb4PerX3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Attribution**\n",
        "***Using the number and size of github commits by author (bar graph), and the git hub\n",
        "visualizations of when the commits occurred. Using these measures each person should\n",
        "self-report how many code-hours of their work are visible in the repo with 2-3 sentences\n",
        "listing their contribution. Do not report any code hours that cannot be traced to commits. If\n",
        "you spend hours on a 2-line change of code or side-reading you did, you cannot report. If\n",
        "you do searches or research for the project that does not result in code, you must create\n",
        "notes in a markdown file (eg. in the project wiki) and the notes should be commensurate\n",
        "with the amount of work reported. Notes cannot be simply copy-pasted from elsewhere\n",
        "(obviously).***"
      ],
      "metadata": {
        "id": "Bks-yjy_tnlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**References**\n",
        "M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The Cityscapes Dataset for Semantic Urban Scene Understanding,” in Proc. of the IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), 2016.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "37t3uj11S__V"
      }
    }
  ]
}